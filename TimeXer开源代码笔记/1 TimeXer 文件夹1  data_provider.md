# TimeXer æ–‡ä»¶å¤¹1  data_provider

## æ–‡ä»¶å¤¹ç»“æ„ï¼š

```
data_provider
â”œâ”€â”€ __init__.py
â”œâ”€â”€ data_factory.py
â”œâ”€â”€ data_loader.py
â”œâ”€â”€ m4.py
â””â”€â”€ uea.py
```

## m4.py

### ä»£ç åŠ æ³¨é‡Š:

å¥½çš„ ğŸ‘
 æˆ‘ä¼šåœ¨ä½ æä¾›çš„ **m4.py** æ–‡ä»¶ä»£ç ä¸­é€æ®µåŠ ä¸Šä¸­æ–‡æ³¨é‡Šï¼Œå¸®åŠ©ä½ ç†è§£æ¯ä¸ªå‡½æ•°ã€ç±»å’Œé€»è¾‘ã€‚

```python
# æœ¬ä»£ç æ¥è‡ª Element AI Inc.ï¼Œå®ç°äº† N-BEATS æ¨¡å‹æ‰€ä½¿ç”¨çš„ M4 æ•°æ®é›†åŠ è½½éƒ¨åˆ†ã€‚
# ç‰ˆæƒå£°æ˜å’Œè®¸å¯ä¿¡æ¯ç•¥ï¼ˆéå•†ä¸šä½¿ç”¨è®¸å¯ï¼‰ã€‚

"""
M4 æ•°æ®é›†å¤„ç†æ¨¡å—
"""
import logging
import os
from collections import OrderedDict
from dataclasses import dataclass
from glob import glob

import numpy as np
import pandas as pd
import patoolib
from tqdm import tqdm
import logging
import os
import pathlib
import sys
from urllib import request


def url_file_name(url: str) -> str:
    """
    ä» URL æå–æ–‡ä»¶å

    :param url: è¾“å…¥çš„ URL åœ°å€
    :return: æå–å‡ºçš„æ–‡ä»¶å
    """
    return url.split('/')[-1] if len(url) > 0 else ''


def download(url: str, file_path: str) -> None:
    """
    ä»æŒ‡å®š URL ä¸‹è½½æ–‡ä»¶åˆ°æœ¬åœ°è·¯å¾„

    :param url: ä¸‹è½½é“¾æ¥
    :param file_path: ä¿å­˜æ–‡ä»¶è·¯å¾„
    """

    def progress(count, block_size, total_size):
        # ä¸‹è½½è¿›åº¦æ¡
        progress_pct = float(count * block_size) / float(total_size) * 100.0
        sys.stdout.write('\rDownloading {} to {} {:.1f}%'.format(url, file_path, progress_pct))
        sys.stdout.flush()

    if not os.path.isfile(file_path):  # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ™ä¸‹è½½
        opener = request.build_opener()
        opener.addheaders = [('User-agent', 'Mozilla/5.0')]  # è®¾ç½®è¯·æ±‚å¤´
        request.install_opener(opener)
        pathlib.Path(os.path.dirname(file_path)).mkdir(parents=True, exist_ok=True)  # ç¡®ä¿æ–‡ä»¶å¤¹å­˜åœ¨
        f, _ = request.urlretrieve(url, file_path, progress)  # æ‰§è¡Œä¸‹è½½ï¼Œå¸¦è¿›åº¦æ¡
        sys.stdout.write('\n')
        sys.stdout.flush()
        file_info = os.stat(f)
        logging.info(f'Successfully downloaded {os.path.basename(file_path)} {file_info.st_size} bytes.')
    else:  # æ–‡ä»¶å·²å­˜åœ¨
        file_info = os.stat(file_path)
        logging.info(f'File already exists: {file_path} {file_info.st_size} bytes.')


@dataclass()
class M4Dataset:
    """
    M4 æ•°æ®é›†ç»“æ„å®šä¹‰
    """
    ids: np.ndarray         # æ—¶é—´åºåˆ—çš„ ID
    groups: np.ndarray      # æ—¶é—´åºåˆ—æ‰€å±çš„ç±»åˆ« (Yearly, Monthly ç­‰)
    frequencies: np.ndarray # æ—¶é—´åºåˆ—çš„é‡‡æ ·é¢‘ç‡
    horizons: np.ndarray    # é¢„æµ‹æ­¥é•¿ï¼ˆhorizonï¼‰
    values: np.ndarray      # åºåˆ—çš„å®é™…å€¼

    @staticmethod
    def load(training: bool = True, dataset_file: str = '../dataset/m4') -> 'M4Dataset':
        """
        åŠ è½½ç¼“å­˜çš„æ•°æ®é›†

        :param training: True è¡¨ç¤ºåŠ è½½è®­ç»ƒé›†ï¼ŒFalse è¡¨ç¤ºåŠ è½½æµ‹è¯•é›†
        :param dataset_file: æ•°æ®é›†æ–‡ä»¶å¤¹è·¯å¾„
        """
        info_file = os.path.join(dataset_file, 'M4-info.csv')       # å­˜æ”¾å…ƒä¿¡æ¯
        train_cache_file = os.path.join(dataset_file, 'training.npz') # è®­ç»ƒé›†ç¼“å­˜æ–‡ä»¶
        test_cache_file = os.path.join(dataset_file, 'test.npz')      # æµ‹è¯•é›†ç¼“å­˜æ–‡ä»¶
        m4_info = pd.read_csv(info_file)  # è¯»å– M4-info.csv æ–‡ä»¶
        return M4Dataset(
            ids=m4_info.M4id.values,
            groups=m4_info.SP.values,
            frequencies=m4_info.Frequency.values,
            horizons=m4_info.Horizon.values,
            values=np.load(train_cache_file if training else test_cache_file, allow_pickle=True)
        )


@dataclass()
class M4Meta:
    """
    M4 æ•°æ®é›†çš„å…ƒä¿¡æ¯å®šä¹‰
    åŒ…æ‹¬ä¸åŒå­£èŠ‚æ€§æ¨¡å¼å¯¹åº”çš„ horizonã€frequency ç­‰
    """
    seasonal_patterns = ['Yearly', 'Quarterly', 'Monthly', 'Weekly', 'Daily', 'Hourly']
    horizons = [6, 8, 18, 13, 14, 48]     # ä¸åŒç±»åˆ«å¯¹åº”çš„é¢„æµ‹é•¿åº¦
    frequencies = [1, 4, 12, 1, 1, 24]    # ä¸åŒç±»åˆ«å¯¹åº”çš„é¢‘ç‡
    horizons_map = {
        'Yearly': 6,
        'Quarterly': 8,
        'Monthly': 18,
        'Weekly': 13,
        'Daily': 14,
        'Hourly': 48
    }  # ä¸åŒå­£èŠ‚æ€§æ¨¡å¼å¯¹åº”çš„é¢„æµ‹é•¿åº¦
    frequency_map = {
        'Yearly': 1,
        'Quarterly': 4,
        'Monthly': 12,
        'Weekly': 1,
        'Daily': 1,
        'Hourly': 24
    }  # ä¸åŒå­£èŠ‚æ€§æ¨¡å¼å¯¹åº”çš„é‡‡æ ·é¢‘ç‡
    history_size = {
        'Yearly': 1.5,
        'Quarterly': 1.5,
        'Monthly': 1.5,
        'Weekly': 10,
        'Daily': 10,
        'Hourly': 10
    }  # å†å²åºåˆ—é•¿åº¦æ¯”ä¾‹ï¼ˆæ¥æºäº interpretable.gin é…ç½®ï¼‰


def load_m4_info() -> pd.DataFrame:
    """
    è¯»å– M4-info æ–‡ä»¶

    :return: Pandas DataFrameï¼ŒåŒ…å« M4 çš„å…ƒä¿¡æ¯
    """
    return pd.read_csv(INFO_FILE_PATH)  # âš ï¸ æ³¨æ„ï¼šINFO_FILE_PATH åœ¨æœ¬æ–‡ä»¶ä¸­æœªå®šä¹‰ï¼Œéœ€è¦åœ¨åˆ«å¤„å®šä¹‰
```

------

è¦ç‚¹æ€»ç»“ï¼š

1. `M4Dataset` ç±»ä¸»è¦è´Ÿè´£ **è¯»å–è®­ç»ƒ/æµ‹è¯•æ•°æ®**ã€‚
2. `M4Meta` ç±»å­˜æ”¾ **ä¸åŒæ—¶é—´åºåˆ—ç±»åˆ«çš„å…ƒä¿¡æ¯**ï¼ˆé¢„æµ‹é•¿åº¦ã€é¢‘ç‡ã€å†å²é•¿åº¦æ¯”ä¾‹ï¼‰ã€‚
3. `download` å‡½æ•°å®ç°äº† **å¸¦è¿›åº¦æ¡çš„æ–‡ä»¶ä¸‹è½½**ã€‚
4. `load_m4_info()` æœ€åç”¨åˆ°çš„ `INFO_FILE_PATH` åœ¨è¿™é‡Œæ²¡æœ‰å®šä¹‰ï¼Œéœ€è¦é¢å¤–é…ç½®ã€‚



### ä»£ç ä½œç”¨

è¿™ä¸ª **`m4.py`** æ–‡ä»¶çš„ä¸»è¦ä½œç”¨å¯ä»¥æ€»ç»“ä¸ºï¼š

å®ƒæ˜¯ **N-BEATS æ¨¡å‹ä»£ç é‡Œä¸“é—¨ç”¨äºå¤„ç† M4 æ•°æ®é›†çš„å·¥å…·æ¨¡å—**ï¼ŒåŠŸèƒ½å¤§æ¦‚åˆ†ä¸‰ç±»ï¼š

------

ğŸ”¹ 1. æ•°æ®ä¸‹è½½ä¸ç®¡ç†

- `download(url, file_path)`ï¼š
   è´Ÿè´£ä»äº’è”ç½‘ä¸Šä¸‹è½½ M4 æ•°æ®é›†çš„å‹ç¼©åŒ…æˆ–ç¼“å­˜æ–‡ä»¶ï¼ˆå¸¦è¿›åº¦æ¡ï¼‰ï¼Œå¹¶ä¿å­˜åˆ°æœ¬åœ°ã€‚

------

ğŸ”¹ 2. æ•°æ®é›†å°è£…ä¸åŠ è½½

- `M4Dataset` ç±»ï¼š
  - å®šä¹‰äº† M4 æ•°æ®é›†çš„åŸºæœ¬ç»“æ„ï¼ˆIDã€åˆ†ç»„ã€é¢‘ç‡ã€é¢„æµ‹æ­¥é•¿ã€æ•°å€¼åºåˆ—ï¼‰ã€‚
  - `M4Dataset.load()`ï¼šå¯ä»¥ä»æœ¬åœ°ç¼“å­˜æ–‡ä»¶é‡Œ **ç›´æ¥è¯»å–è®­ç»ƒé›†æˆ–æµ‹è¯•é›†**ï¼Œå¹¶è¿”å›ä¸€ä¸ª `M4Dataset` å¯¹è±¡ã€‚
  - è¿™æ ·åšçš„å¥½å¤„æ˜¯æ¨¡å‹è®­ç»ƒæ—¶ä¸éœ€è¦æ¯æ¬¡éƒ½å»è§£æåŸå§‹ CSVï¼Œè€Œæ˜¯ç”¨ `.npz` ç¼“å­˜åŠ å¿«é€Ÿåº¦ã€‚

------

ğŸ”¹ 3. å…ƒä¿¡æ¯é…ç½®

- `M4Meta` ç±»ï¼š
  - å®šä¹‰äº† M4 ä¸åŒæ—¶é—´åºåˆ—ç±»å‹ï¼ˆYearly, Quarterly, Monthly, Weekly, Daily, Hourlyï¼‰çš„
    - é¢„æµ‹æ­¥é•¿ï¼ˆhorizonï¼‰
    - é‡‡æ ·é¢‘ç‡ï¼ˆfrequencyï¼‰
    - å†å²é•¿åº¦æ¯”ä¾‹ï¼ˆhistory sizeï¼‰
  - è¿™äº›ä¿¡æ¯åœ¨è®­ç»ƒæ¨¡å‹æ—¶ä¼šç”¨åˆ°ï¼Œæ¯”å¦‚å†³å®šè¾“å…¥çª—å£é•¿åº¦ã€é¢„æµ‹åŒºé—´é•¿åº¦ã€‚

### m4æ•°æ®é›†

M4 æ•°æ®é›†ï¼ˆM4 Competition Datasetï¼‰æ˜¯ **ä¸€ä¸ªæ—¶é—´åºåˆ—é¢„æµ‹ç«èµ›æ•°æ®é›†**ï¼Œé‡Œé¢åŒ…å«äº† **10ä¸‡æ¡ä»¥ä¸Š** çš„ä¸åŒæ—¶é—´åºåˆ—æ•°æ®ï¼ˆæŒ‰ Yearly, Quarterly, Monthly, Weekly, Daily, Hourly å…­ç±»åˆ’åˆ†ï¼‰ã€‚

æ¯ä¸€æ¡æ—¶é—´åºåˆ—éƒ½æœ‰ï¼š

- **ID**ï¼ˆåºåˆ—ç¼–å·ï¼‰
- **ç±»åˆ«**ï¼ˆå­£èŠ‚æ€§æ¨¡å¼ï¼Œæ¯”å¦‚ Yearlyï¼‰
- **é¢‘ç‡**ï¼ˆ1 è¡¨ç¤ºå¹´ï¼Œ12 è¡¨ç¤ºæœˆï¼Œ24 è¡¨ç¤ºå°æ—¶ç­‰ï¼‰
- **å†å²æ•°æ®å€¼**ï¼ˆè®­ç»ƒæ•°æ®ï¼‰
- **é¢„æµ‹æ­¥é•¿ Horizon**ï¼ˆæœªæ¥è¦é¢„æµ‹å¤šå°‘ç‚¹ï¼Œæ¯”å¦‚æœˆåº¦åºåˆ—è¦é¢„æµ‹ 18 ä¸ªæœˆï¼‰

------

ğŸ“Š ä¸¾ä¸ªå…·ä½“ä¾‹å­ï¼ˆMonthly åºåˆ—ï¼‰

å‡è®¾æœ‰ä¸€ä¸ªæœˆåº¦çš„åºåˆ—ï¼š

| M4id | Category | Frequency | Horizon | Values (å†å²åºåˆ—)            |
| ---- | -------- | --------- | ------- | ---------------------------- |
| Y1   | Monthly  | 12        | 18      | [266, 145, 183, 119, 180, â€¦] |

- **M4id**: `Y1` ï¼ˆåºåˆ—çš„å”¯ä¸€æ ‡è¯†ç¬¦ï¼‰
- **Category**: `Monthly` ï¼ˆæœˆåº¦åºåˆ—ï¼‰
- **Frequency**: `12` ï¼ˆä¸€å¹´ 12 ä¸ªç‚¹ï¼‰
- **Horizon**: `18` ï¼ˆéœ€è¦é¢„æµ‹æœªæ¥ 18 ä¸ªæœˆçš„å€¼ï¼‰
- **Values**: `[266, 145, 183, 119, 180, â€¦]` ï¼ˆä»è¿‡å»æŸå¹´åˆ°ç°åœ¨çš„æœˆåº¦è§‚æµ‹å€¼ï¼‰

------

ğŸ“Š å†ä¸¾ä¸ª Yearly åºåˆ—çš„ä¾‹å­

| M4id | Category | Frequency | Horizon | Values (å†å²åºåˆ—)            |
| ---- | -------- | --------- | ------- | ---------------------------- |
| Y10  | Yearly   | 1         | 6       | [123, 150, 160, 180, 200, â€¦] |

- **Frequency = 1**ï¼Œè¡¨ç¤ºæ¯å¹´ä¸€ä¸ªç‚¹
- **Horizon = 6**ï¼Œè¡¨ç¤ºè¦é¢„æµ‹æœªæ¥ 6 å¹´
- **Values** æ˜¯è¿™ä¸ªåºåˆ—è¿‡å»å‡ åå¹´çš„æ•°å€¼ï¼ˆæ¯”å¦‚ç»æµæŒ‡æ ‡ã€é”€é‡ã€èƒ½æºæ¶ˆè€—ç­‰ï¼‰

------

ğŸ“Œ æ€»ç»“ä¸€ä¸‹ï¼š
 M4 æ•°æ®é›†å°±æ˜¯ä¸€ä¸ª **å¤§é‡æ—¶é—´åºåˆ—çš„é›†åˆ**ï¼Œæ¯ä¸ªåºåˆ—æœ‰ **å†å²å€¼** + **ç±»åˆ«å…ƒä¿¡æ¯** + **æœªæ¥é¢„æµ‹æ­¥é•¿**ã€‚
 N-BEATS æˆ–å…¶å®ƒæ¨¡å‹çš„ä»»åŠ¡å°±æ˜¯ **å­¦ä¼šä»å†å²å€¼é¢„æµ‹ Horizon ä¸ªæœªæ¥å€¼**ã€‚

## uea.py

### ä»£ç åŠ æ³¨é‡Š

å¥½çš„ ğŸ‘
 æˆ‘åœ¨ä½ ç»™çš„ä»£ç åŸºç¡€ä¸Šï¼Œé€è¡ŒåŠ ä¸Šä¸­æ–‡æ³¨é‡Šï¼Œå¸®ä½ æ¢³ç†æ¯ä¸ªå‡½æ•°å’Œç±»çš„ä½œç”¨ï¼š

```python
import os
import numpy as np
import pandas as pd
import torch


def collate_fn(data, max_len=None):
    """
    æ„å»ºä¸€ä¸ª batch çš„å¼ é‡ï¼ˆä¸»è¦ç”¨äº DataLoader çš„ collate_fn å‡½æ•°ï¼‰

    Args:
        data: ä¸€ä¸ª batch çš„æ ·æœ¬åˆ—è¡¨ï¼Œé•¿åº¦ä¸º batch_size
              æ¯ä¸ªæ ·æœ¬æ˜¯ (X, y) äºŒå…ƒç»„
                - X: torch.tensorï¼Œå½¢çŠ¶ (seq_length, feat_dim)ï¼Œåºåˆ—é•¿åº¦å¯ä»¥ä¸åŒ
                - y: torch.tensorï¼Œå½¢çŠ¶ (num_labels,)ï¼Œæ ‡ç­¾ï¼ˆåˆ†ç±»/å›å½’ä»»åŠ¡ï¼‰
        max_len: å…¨å±€å›ºå®šçš„åºåˆ—é•¿åº¦
                 - å¦‚æœæ¨¡å‹éœ€è¦å›ºå®šé•¿åº¦è¾“å…¥ï¼Œå°±ç”¨è¿™ä¸ªå‚æ•°
                 - å¦‚æœåºåˆ—å¤ªé•¿åˆ™æˆªæ–­ï¼Œå¤ªçŸ­åˆ™è¡¥é›¶

    Returns:
        X: (batch_size, padded_length, feat_dim) è¾“å…¥ç‰¹å¾å¼ é‡ï¼ˆå¡«å……åï¼‰
        targets: (batch_size, num_labels) æ ‡ç­¾å¼ é‡
        padding_masks: (batch_size, padded_length) å¸ƒå°”å¼ é‡
            - 1 è¡¨ç¤ºçœŸå®å€¼ï¼Œ0 è¡¨ç¤ºè¡¥é›¶
    """

    batch_size = len(data)
    features, labels = zip(*data)  # è§£å‹å¾—åˆ°ç‰¹å¾åˆ—è¡¨å’Œæ ‡ç­¾åˆ—è¡¨

    # æ¯ä¸ªæ ·æœ¬åŸå§‹åºåˆ—çš„é•¿åº¦
    lengths = [X.shape[0] for X in features]
    if max_len is None:
        max_len = max(lengths)  # å¦‚æœæ²¡æŒ‡å®š max_lenï¼Œåˆ™ç”¨å½“å‰ batch çš„æœ€å¤§é•¿åº¦

    # åˆå§‹åŒ–ç‰¹å¾å¼ é‡ï¼Œé»˜è®¤å¡« 0
    X = torch.zeros(batch_size, max_len, features[0].shape[-1])
    for i in range(batch_size):
        end = min(lengths[i], max_len)  # é¿å…è¶…å‡º max_len
        X[i, :end, :] = features[i][:end, :]  # æ‹·è´æ•°æ®åˆ° batch å¼ é‡ä¸­

    # å°†æ ‡ç­¾å †å æˆ (batch_size, num_labels) å½¢çŠ¶
    targets = torch.stack(labels, dim=0)

    # æ„é€  padding æ©ç ï¼Œæ ‡è®°å“ªäº›ä½ç½®æ˜¯æœ‰æ•ˆçš„
    padding_masks = padding_mask(torch.tensor(lengths, dtype=torch.int16),
                                 max_len=max_len)

    return X, targets, padding_masks


def padding_mask(lengths, max_len=None):
    """
    æ ¹æ®åºåˆ—é•¿åº¦ç”Ÿæˆ padding æ©ç 

    Args:
        lengths: å¼ é‡ï¼Œå½¢çŠ¶ (batch_size,) ï¼Œè¡¨ç¤ºæ¯ä¸ªæ ·æœ¬çš„çœŸå®é•¿åº¦
        max_len: æ‰¹æ¬¡ä¸­åºåˆ—çš„æœ€å¤§é•¿åº¦

    Returns:
        mask: (batch_size, max_len) çš„å¸ƒå°”æ©ç å¼ é‡
              - 1 è¡¨ç¤ºä¿ç•™ï¼ˆçœŸå®æ•°æ®ï¼‰
              - 0 è¡¨ç¤º paddingï¼ˆè¡¥é›¶ï¼‰
    """
    batch_size = lengths.numel()
    max_len = max_len or lengths.max()  # å¦‚æœæ²¡ä¼ å…¥ï¼Œå°±ç”¨æœ€å¤§åºåˆ—é•¿åº¦
    return (torch.arange(0, max_len, device=lengths.device)
            .type_as(lengths)                 # ä¿æŒ dtype ä¸€è‡´
            .repeat(batch_size, 1)            # æ‰©å±•ä¸º (batch_size, max_len)
            .lt(lengths.unsqueeze(1)))        # å°äºçœŸå®é•¿åº¦çš„ä¸º 1ï¼Œå¦åˆ™ä¸º 0


class Normalizer(object):
    """
    æ•°æ®å½’ä¸€åŒ–å·¥å…·ç±»
    - å¯ä»¥å¯¹æ•´ä¸ª DataFrame æˆ–æ¯ä¸ªæ ·æœ¬å•ç‹¬åšæ ‡å‡†åŒ– / min-max å½’ä¸€åŒ–
    """

    def __init__(self, norm_type='standardization', mean=None, std=None, min_val=None, max_val=None):
        """
        Args:
            norm_type: å½’ä¸€åŒ–æ–¹å¼
                "standardization"   -> å…¨å±€æ ‡å‡†åŒ–
                "minmax"            -> å…¨å±€ min-max å½’ä¸€åŒ–
                "per_sample_std"    -> æ¯ä¸ªæ ·æœ¬å•ç‹¬æ ‡å‡†åŒ–
                "per_sample_minmax" -> æ¯ä¸ªæ ·æœ¬å•ç‹¬ min-max å½’ä¸€åŒ–
            mean, std, min_val, max_val: å¯é€‰ï¼Œé¢„å…ˆè®¡ç®—å¥½çš„å‚æ•°
        """

        self.norm_type = norm_type
        self.mean = mean
        self.std = std
        self.min_val = min_val
        self.max_val = max_val

    def normalize(self, df):
        """
        å¯¹ DataFrame åšå½’ä¸€åŒ–

        Args:
            df: è¾“å…¥çš„ pandas.DataFrame
        Returns:
            df: å½’ä¸€åŒ–åçš„ DataFrame
        """
        if self.norm_type == "standardization":
            # å…¨å±€æ ‡å‡†åŒ– (x - mean) / std
            if self.mean is None:
                self.mean = df.mean()
                self.std = df.std()
            return (df - self.mean) / (self.std + np.finfo(float).eps)

        elif self.norm_type == "minmax":
            # å…¨å±€ min-max å½’ä¸€åŒ–
            if self.max_val is None:
                self.max_val = df.max()
                self.min_val = df.min()
            return (df - self.min_val) / (self.max_val - self.min_val + np.finfo(float).eps)

        elif self.norm_type == "per_sample_std":
            # æ¯ä¸ªæ ·æœ¬å•ç‹¬æ ‡å‡†åŒ–
            grouped = df.groupby(by=df.index)
            return (df - grouped.transform('mean')) / grouped.transform('std')

        elif self.norm_type == "per_sample_minmax":
            # æ¯ä¸ªæ ·æœ¬å•ç‹¬ min-max å½’ä¸€åŒ–
            grouped = df.groupby(by=df.index)
            min_vals = grouped.transform('min')
            return (df - min_vals) / (grouped.transform('max') - min_vals + np.finfo(float).eps)

        else:
            raise (NameError(f'Normalize method "{self.norm_type}" not implemented'))


def interpolate_missing(y):
    """
    çº¿æ€§æ’å€¼å¡«è¡¥ç¼ºå¤±å€¼

    Args:
        y: pd.Seriesï¼Œè¾“å…¥åºåˆ—
    Returns:
        y: å¡«è¡¥ç¼ºå¤±å€¼åçš„åºåˆ—
    """
    if y.isna().any():
        y = y.interpolate(method='linear', limit_direction='both')
    return y


def subsample(y, limit=256, factor=2):
    """
    å­é‡‡æ ·å‡½æ•°
    - å¦‚æœåºåˆ—é•¿åº¦å¤§äº limitï¼Œå°±æŒ‰æ­¥é•¿ factor é‡‡æ ·
    - å¦åˆ™åŸæ ·è¿”å›

    Args:
        y: pd.Seriesï¼Œè¾“å…¥åºåˆ—
        limit: é™åˆ¶é•¿åº¦
        factor: å­é‡‡æ ·æ­¥é•¿
    Returns:
        y: å­é‡‡æ ·åçš„åºåˆ—
    """
    if len(y) > limit:
        return y[::factor].reset_index(drop=True)
    return y
```

------

ğŸ”‘ åŠŸèƒ½æ€»ç»“

1. **`collate_fn` + `padding_mask`**
   - ç”¨äºæ„é€ ä¸€ä¸ª batch çš„è¾“å…¥ï¼Œæ”¯æŒä¸åŒé•¿åº¦åºåˆ—ï¼Œç»Ÿä¸€æˆå›ºå®šå½¢çŠ¶ï¼ˆpaddingï¼‰ã€‚
2. **`Normalizer`**
   - æä¾›ä¸åŒçš„å½’ä¸€åŒ–æ–¹æ³•ï¼ˆå…¨å±€/é€æ ·æœ¬çš„æ ‡å‡†åŒ–å’Œ min-maxï¼‰ã€‚
3. **`interpolate_missing`**
   - ç”¨æ’å€¼æ–¹æ³•å¡«è¡¥ç¼ºå¤±å€¼ã€‚
4. **`subsample`**
   - å¦‚æœåºåˆ—å¤ªé•¿ï¼Œå¯ä»¥å­é‡‡æ ·ä»¥å‡å°‘è®¡ç®—é‡ã€‚

------

### ä»£ç ä½œç”¨

æ˜ç™½äº† âœ…

è¿™ä¸€æ®µä»£ç çš„**ä¸»è¦ä½œç”¨**å¯ä»¥æ€»ç»“ä¸ºï¼š



è¿™æ˜¯ä¸€ä¸ª **æ—¶é—´åºåˆ—æ•°æ®é¢„å¤„ç†å·¥å…·æ¨¡å—**ï¼Œå®ƒçš„èŒè´£æ˜¯æŠŠåŸå§‹çš„æ—¶é—´åºåˆ—æ•´ç†æˆèƒ½ç›´æ¥é€è¿›ç¥ç»ç½‘ç»œæ¨¡å‹çš„å¼ é‡ï¼ŒåŒæ—¶æä¾›ä¸€äº›å¸¸ç”¨çš„æ•°æ®æ¸…æ´—ä¸å½’ä¸€åŒ–æ–¹æ³•ã€‚

1. **æ‰¹å¤„ç†æ‹¼æ¥ (`collate_fn` + `padding_mask`)**

   - è§£å†³æ—¶é—´åºåˆ— **é•¿çŸ­ä¸ä¸€** çš„é—®é¢˜
   - æŠŠä¸€æ‰¹ `(X, y)` æ•°æ®å¯¹æ•´ç†æˆç»Ÿä¸€å½¢çŠ¶çš„ä¸‰ç»´å¼ é‡ `(batch_size, padded_len, feat_dim)`
   - å¯¹çŸ­åºåˆ—è¡¥é›¶ï¼ˆpaddingï¼‰ï¼Œå¯¹é•¿åºåˆ—æˆªæ–­ï¼Œå¹¶ç”Ÿæˆæ©ç æ ‡è®°å“ªäº›æ˜¯çœŸå®å€¼ã€å“ªäº›æ˜¯è¡¥é›¶

   ğŸ‘‰ ä¸»è¦ç”¨äº **PyTorch DataLoader** çš„ `collate_fn` å‚æ•°

------

1. **æ•°æ®å½’ä¸€åŒ– (`Normalizer`)**
   - æä¾› 4 ç§å½’ä¸€åŒ–æ–¹å¼ï¼š
     - å…¨å±€æ ‡å‡†åŒ–
     - å…¨å±€ min-max
     - æŒ‰æ ·æœ¬æ ‡å‡†åŒ–
     - æŒ‰æ ·æœ¬ min-max
   - ä½œç”¨æ˜¯æ¶ˆé™¤ä¸åŒç‰¹å¾é‡çº§å·®å¼‚ï¼Œè®©æ¨¡å‹æ›´å®¹æ˜“æ”¶æ•›

------

1. **ç¼ºå¤±å€¼å¤„ç† (`interpolate_missing`)**
   - ç”¨ **çº¿æ€§æ’å€¼** è‡ªåŠ¨å¡«è¡¥æ—¶é—´åºåˆ—é‡Œçš„ NaN ç¼ºå¤±ç‚¹

------

1. **åºåˆ—å‹ç¼© (`subsample`)**
   - å¦‚æœæ—¶é—´åºåˆ—å¤ªé•¿ï¼ˆ> limitï¼‰ï¼Œå°±æŒ‰ç…§ `factor` è¿›è¡Œå­é‡‡æ ·
   - è¿™æ ·å¯ä»¥å‡å°‘è®¡ç®—é‡ï¼Œæé«˜è®­ç»ƒé€Ÿåº¦

------



## data_loader.py



### ä»£ç åŠ æ³¨é‡Š

ï¼ˆä»£ç å¤ªé•¿äº†ï¼šé“¾æ¥åœ¨ï¼šâ€œ[TimeXer/data_provider/data_loader.py at main Â· thuml/TimeXer Â· GitHub](https://github.com/thuml/TimeXer/blob/main/data_provider/data_loader.py)â€ï¼‰



### ä»£ç ä½œç”¨

**ä»£ç çš„ä¸»ä»£ç çš„ä¸»è¦ä½œç”¨**

1. **ç»Ÿä¸€æ•°æ®è¯»å–æ¥å£**
   - é’ˆå¯¹ä¸åŒæ¥æºçš„æ•°æ®é›†ï¼ˆETTã€M4ã€PSMã€MSLã€SMAPã€SMDã€SWATã€UEAç­‰ï¼‰ï¼Œå®šä¹‰äº†å¯¹åº”çš„ Dataset ç±»ã€‚
   - ä¸åŒæ•°æ®é›†çš„æ–‡ä»¶æ ¼å¼ã€æ—¶é—´ç²’åº¦ã€åˆ‡åˆ†æ–¹å¼ä¸åŒï¼Œè¿™äº›ç±»å±è”½äº†å·®å¼‚ï¼Œç»Ÿä¸€è¾“å‡ºè®­ç»ƒæ‰€éœ€çš„ `(seq_x, seq_y, seq_x_mark, seq_y_mark)` æˆ– `(çª—å£æ•°æ®, æ ‡ç­¾)`ã€‚
2. **æ•°æ®é¢„å¤„ç†**
   - ä½¿ç”¨ `StandardScaler` è¿›è¡Œå½’ä¸€åŒ–ï¼ˆé˜²æ­¢ç‰¹å¾å°ºåº¦å·®å¼‚å½±å“è®­ç»ƒï¼‰ã€‚
   - ç¼ºå¤±å€¼å¡«å…… (`np.nan_to_num`)ã€‚
   - æå–æ—¶é—´ç‰¹å¾ï¼ˆå¹´æœˆæ—¥ã€å°æ—¶ã€åˆ†é’Ÿã€weekday ç­‰ï¼Œæˆ–é€šè¿‡ `time_features` æå–é¢‘ç‡ç¼–ç ï¼‰ã€‚
3. **æ—¶é—´åºåˆ—åˆ‡ç‰‡**
   - è¾“å…¥ï¼ˆå†å²çª—å£ `seq_len`ï¼‰å’Œè¾“å‡ºï¼ˆæ ‡ç­¾çª—å£ `label_len` + é¢„æµ‹çª—å£ `pred_len`ï¼‰ã€‚
   - ä¿è¯åœ¨è®­ç»ƒã€éªŒè¯ã€æµ‹è¯•é˜¶æ®µä½¿ç”¨ä¸åŒçš„æ—¶é—´æ®µã€‚
4. **æ”¯æŒæ•°æ®å¢å¼º**
   - å¦‚æœ `args.augmentation_ratio > 0`ï¼Œåœ¨è®­ç»ƒé›†ä¸Šè¿è¡Œ `run_augmentation_single`ï¼Œæ‰©å……æ•°æ®ã€‚
5. **å…¼å®¹ä¸åŒä»»åŠ¡**
   - **é¢„æµ‹ä»»åŠ¡**ï¼ˆETTh1/ETTm1/Custom/M4ï¼‰ï¼šè¾“å‡ºå¸¦æ—¶é—´æ ‡è®°çš„è¾“å…¥å’Œé¢„æµ‹çª—å£ã€‚
   - **å¼‚å¸¸æ£€æµ‹/åˆ†å‰²ä»»åŠ¡**ï¼ˆPSMã€MSLã€SMAPã€SMDã€SWATï¼‰ï¼šæŒ‰å›ºå®šæ»‘åŠ¨çª—å£åˆ’åˆ†åºåˆ—ï¼Œè¿”å›æ•°æ®ä¸å¯¹åº”æ ‡ç­¾ã€‚
   - **åˆ†ç±»ä»»åŠ¡**ï¼ˆUEAï¼‰ï¼šåŠ è½½ UEA Archive æ•°æ®é›†ï¼Œè¿”å›åºåˆ—ä¸æ ‡ç­¾ã€‚

------

**å„ç±» Dataset çš„è¯´æ˜**

1. **`Dataset_ETT_hour` / `Dataset_ETT_minute`**
   - ç”¨äº ETT æ•°æ®é›†ï¼ˆç”µåŠ›å˜å‹å™¨è´Ÿè·æ•°æ®ï¼‰ï¼Œå°æ—¶çº§å’Œåˆ†é’Ÿçº§ã€‚
   - æŒ‰ç…§è®ºæ–‡é»˜è®¤æ¯”ä¾‹åˆ’åˆ† **è®­ç»ƒï¼šéªŒè¯ï¼šæµ‹è¯• = 12:4:4 æœˆ**ã€‚
2. **`Dataset_Custom`**
   - ç”¨äºè‡ªå®šä¹‰ CSV æ•°æ®é›†ï¼ˆç¬¬ä¸€åˆ—ä¸º dateï¼Œæœ€åä¸€åˆ—ä¸º targetï¼‰ã€‚
   - æŒ‰æ¯”ä¾‹åˆ‡åˆ†ï¼š70% è®­ç»ƒï¼Œ20% æµ‹è¯•ï¼Œ10% éªŒè¯ã€‚
3. **`Dataset_M4`**
   - M4 æ—¶é—´åºåˆ—é¢„æµ‹ç«èµ›æ•°æ®é›†ã€‚
   - æŒ‰é¢‘ç‡ï¼ˆYearlyã€Quarterlyã€Monthlyç­‰ï¼‰æŠ½å–åºåˆ—ï¼Œç”Ÿæˆ in-sample å’Œ out-sample çª—å£ã€‚
4. **`PSMSegLoader`ã€`MSLSegLoader`ã€`SMAPSegLoader`ã€`SMDSegLoader`ã€`SWATSegLoader`**
   - é’ˆå¯¹å·¥ä¸šè¿‡ç¨‹/ä¼ æ„Ÿå™¨çš„ **å¼‚å¸¸æ£€æµ‹æ•°æ®é›†**ã€‚
   - æŒ‰æ»‘åŠ¨çª—å£ç”Ÿæˆè¾“å…¥åºåˆ—ï¼Œå¹¶è¿”å›å¯¹åº”çš„å¼‚å¸¸æ ‡ç­¾ã€‚
5. **`UEAloader`**
   - åŠ è½½ UEA æ—¶é—´åºåˆ—åˆ†ç±»æ•°æ®é›†ã€‚
   - æ”¯æŒé€‰æ‹©éƒ¨åˆ†æ–‡ä»¶ã€é™åˆ¶æ ·æœ¬æ•°ã€å½’ä¸€åŒ–ç­‰ã€‚



## data_factory.py



### ğŸ”‘ä»£ç åŠ æ³¨é‡Š

è¿™æ®µä»£ç å…¶å®å°±æ˜¯ä¸€ä¸ª **æ•°æ®å·¥å‚å‡½æ•° (data_provider)**ï¼Œæ ¹æ®é…ç½®å‚æ•°è‡ªåŠ¨é€‰æ‹©å¹¶æ„é€ ä¸åŒçš„æ•°æ®é›†ï¼ˆDatasetï¼‰å’Œå¯¹åº”çš„ DataLoaderã€‚
 æˆ‘ç»™ä½ é€æ­¥åŠ ä¸Šä¸­æ–‡æ³¨é‡Šå’Œä½œç”¨è¯´æ˜ã€‚

------

```
from data_provider.data_loader import (
    Dataset_ETT_hour, Dataset_ETT_minute, Dataset_Custom, Dataset_M4,
    PSMSegLoader, MSLSegLoader, SMAPSegLoader, SMDSegLoader, SWATSegLoader,
    UEAloader, Dataset_Meteorology
)
from data_provider.uea import collate_fn
from torch.utils.data import DataLoader


# =============================================
# æ•°æ®é›†å­—å…¸ - ä»»åŠ¡å -> æ•°æ®é›†ç±»
# =============================================
data_dict = {
    'ETTh1': Dataset_ETT_hour,      # ç”µåŠ›è´Ÿè· (å°æ—¶çº§)
    'ETTh2': Dataset_ETT_hour,
    'ETTm1': Dataset_ETT_minute,    # ç”µåŠ›è´Ÿè· (åˆ†é’Ÿçº§)
    'ETTm2': Dataset_ETT_minute,
    'custom': Dataset_Custom,       # è‡ªå®šä¹‰ CSV
    'm4': Dataset_M4,               # M4 ç«èµ›æ•°æ®
    'PSM': PSMSegLoader,            # å¼‚å¸¸æ£€æµ‹ (PSM)
    'MSL': MSLSegLoader,            # å¼‚å¸¸æ£€æµ‹ (MSL)
    'SMAP': SMAPSegLoader,          # å¼‚å¸¸æ£€æµ‹ (SMAP)
    'SMD': SMDSegLoader,            # å¼‚å¸¸æ£€æµ‹ (SMD)
    'SWAT': SWATSegLoader,          # å¼‚å¸¸æ£€æµ‹ (SWAT)
    'UEA': UEAloader,               # UEA æ—¶é—´åºåˆ—åˆ†ç±»
    'Meteorology': Dataset_Meteorology  # æ°”è±¡æ•°æ®
}


# =============================================
# æ•°æ®å·¥å‚å‡½æ•°
# æ ¹æ®ä»»åŠ¡ç±»å‹å’Œæ•°æ®é›†é€‰æ‹©åˆé€‚çš„ Dataset & DataLoader
# =============================================
def data_provider(args, flag):
    # æ ¹æ®å‚æ•° args.data é€‰æ‹©æ•°æ®é›†ç±»
    Data = data_dict[args.data]

    # æ—¶é—´ç‰¹å¾ç¼–ç æ–¹å¼
    # å¦‚æœ embedding æ–¹å¼æ˜¯ "timeF"ï¼Œåˆ™ä½¿ç”¨åµŒå…¥å¼ç¼–ç  (1)ï¼Œå¦åˆ™ç”¨ä¼ ç»Ÿæ—¶é—´åˆ†è§£ (0)
    timeenc = 0 if args.embed != 'timeF' else 1

    # DataLoader é…ç½®
    shuffle_flag = False if (flag == 'test' or flag == 'TEST') else True  # æµ‹è¯•é›†ä¸æ‰“ä¹±
    drop_last = False                                                    # é»˜è®¤ä¸ä¸¢å¼ƒæœ€åä¸€ä¸ª batch
    batch_size = args.batch_size
    freq = args.freq  # é‡‡æ ·é¢‘ç‡ (h/m/d)

    # ========== 1. å¼‚å¸¸æ£€æµ‹ä»»åŠ¡ ==========
    if args.task_name == 'anomaly_detection':
        drop_last = False
        data_set = Data(
            args=args,
            root_path=args.root_path,
            win_size=args.seq_len,   # çª—å£å¤§å° (å¼‚å¸¸æ£€æµ‹ç”¨å›ºå®šçª—å£)
            flag=flag,
        )
        print(flag, len(data_set))   # æ‰“å°æ•°æ®é›†è§„æ¨¡
        data_loader = DataLoader(
            data_set,
            batch_size=batch_size,
            shuffle=shuffle_flag,
            num_workers=args.num_workers,
            drop_last=drop_last
        )
        return data_set, data_loader

    # ========== 2. åˆ†ç±»ä»»åŠ¡ ==========
    elif args.task_name == 'classification':
        drop_last = False
        data_set = Data(
            args=args,
            root_path=args.root_path,
            flag=flag,
        )

        data_loader = DataLoader(
            data_set,
            batch_size=batch_size,
            shuffle=shuffle_flag,
            num_workers=args.num_workers,
            drop_last=drop_last,
            collate_fn=lambda x: collate_fn(x, max_len=args.seq_len)  
            # åˆ†ç±»ä»»åŠ¡éœ€è¦ç‰¹æ®Šçš„ collate_fnï¼ˆå˜é•¿åºåˆ—å¯¹é½ & maskï¼‰
        )
        return data_set, data_loader

    # ========== 3. æ—¶é—´åºåˆ—é¢„æµ‹ä»»åŠ¡ ==========
    else:
        if args.data == 'm4':  # M4 æ•°æ®é›†ä¸ä¸¢å¼ƒæœ€å batch
            drop_last = False

        data_set = Data(
            args=args,
            root_path=args.root_path,
            data_path=args.data_path,
            flag=flag,
            size=[args.seq_len, args.label_len, args.pred_len],  # çª—å£é•¿åº¦é…ç½®
            features=args.features,         # 'M' (å¤šå˜é‡) / 'S' (å•å˜é‡)
            target=args.target,             # é¢„æµ‹ç›®æ ‡åˆ—
            timeenc=timeenc,                # æ—¶é—´ç‰¹å¾ç¼–ç æ–¹å¼
            freq=freq,                      # æ•°æ®é¢‘ç‡
            seasonal_patterns=args.seasonal_patterns  # å­£èŠ‚æ¨¡å¼ (M4 ç‰¹æœ‰)
        )
        print(flag, len(data_set))
        data_loader = DataLoader(
            data_set,
            batch_size=batch_size,
            shuffle=shuffle_flag,
            num_workers=args.num_workers,
            drop_last=drop_last
        )
        return data_set, data_loader
```

------

### ğŸ”‘ä»£ç ä½œç”¨

è¿™æ®µä»£ç å°±æ˜¯ä¸€ä¸ª **ç»Ÿä¸€æ•°æ®å…¥å£**ï¼š

1. **æ ¹æ®æ•°æ®é›†åç§° (`args.data`)** ä» `data_dict` é€‰æ‹©å¯¹åº”çš„ `Dataset` ç±»ã€‚
2. **æ ¹æ®ä»»åŠ¡ç±»å‹ (`args.task_name`)** æ„å»ºæ•°æ®é›†ï¼š
   - **é¢„æµ‹ (forecasting)** â†’ è¿”å›é¢„æµ‹ç”¨ Dataset + DataLoader
   - **åˆ†ç±» (classification)** â†’ ä½¿ç”¨ `collate_fn` å¤„ç†å˜é•¿åºåˆ—
   - **å¼‚å¸¸æ£€æµ‹ (anomaly_detection)** â†’ ä½¿ç”¨å›ºå®šçª—å£ Dataset
3. **è‡ªåŠ¨é…ç½® DataLoader**ï¼ˆbatch_size, shuffle, drop_last, num_workersï¼‰ã€‚

è¿™æ ·åšçš„å¥½å¤„æ˜¯ï¼š
 ğŸ‘‰ åœ¨è®­ç»ƒè„šæœ¬é‡Œï¼Œåªéœ€è¦å†™ `train_data, train_loader = data_provider(args, 'train')`ï¼Œä¸ç”¨å…³å¿ƒå…·ä½“çš„æ•°æ®é›†å·®å¼‚ã€‚









