# utils ç½‘ç»œç»„ä»¶ä»£ç åº“è®²è§£ï¼š

ä½ è´´çš„è¿™ä¸ªè·¯å¾„å’Œæäº¤è®°å½•æ˜¾ç¤ºçš„æ˜¯ **DUET ä»“åº“é‡Œ `ts_benchmark/baselines/duet/utils/` ç›®å½•** çš„å†…å®¹ã€‚è¿™ä¸ªç›®å½•ä¸»è¦å­˜æ”¾ä¸€äº›è¾…åŠ©å·¥å…·å’ŒåŸºç¡€åŠŸèƒ½æ–‡ä»¶ï¼Œä¸æ˜¯æ¨¡å‹ä¸»ä½“ï¼Œè€Œæ˜¯æ¨¡å‹è®­ç»ƒå’Œå®ç°æ—¶ç”¨åˆ°çš„å·¥å…·å‡½æ•°ï¼Œæ¯”å¦‚æ—¶é—´ç‰¹å¾å¤„ç†ã€æ©ç ã€æ³¨æ„åŠ›ç­‰ã€‚å…·ä½“è¯´æ˜å¦‚ä¸‹ï¼š

| æ–‡ä»¶                  | åŠŸèƒ½è¯´æ˜                                                     |
| --------------------- | ------------------------------------------------------------ |
| `__init__.py`         | Python åŒ…åˆå§‹åŒ–æ–‡ä»¶ï¼Œä½¿ `utils` ç›®å½•å¯è¢«å½“ä½œæ¨¡å—å¯¼å…¥ã€‚       |
| `losses.py`           | å®šä¹‰è®­ç»ƒæ—¶çš„æŸå¤±å‡½æ•°ï¼Œæ¯”å¦‚é¢„æµ‹è¯¯å·®ã€åŠ æƒæŸå¤±ç­‰ã€‚             |
| `masked_attention.py` | å®ç°å¸¦æ©ç çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œç”¨äºå¤„ç†å˜é•¿åºåˆ—æˆ–è€…æœªæ¥ä¿¡æ¯å±è”½ï¼ˆcausal attentionï¼‰ã€‚ |
| `masking.py`          | å®šä¹‰å„ç§æ©ç ç”Ÿæˆå‡½æ•°ï¼Œæ¯”å¦‚æ—¶é—´åºåˆ—çš„ padding mask æˆ– causal maskã€‚ |
| `timefeatures.py`     | æä¾›æ—¶é—´ç‰¹å¾æå–å‡½æ•°ï¼ŒæŠŠæ—¥æœŸæ—¶é—´ä¿¡æ¯ç¼–ç æˆæ¨¡å‹å¯ç”¨çš„å‘é‡ã€‚   |
| `tools.py`            | ä¸€äº›é€šç”¨å·¥å…·å‡½æ•°ï¼Œæ¯”å¦‚å¼ é‡å¤„ç†ã€æ•°æ®è½¬æ¢ã€è°ƒè¯•è¾…åŠ©å‡½æ•°ç­‰ã€‚   |

ç®€å•æ€»ç»“ï¼š**è¿™ä¸ª `utils` æ–‡ä»¶å¤¹æ˜¯ DUET æ¨¡å‹çš„è¾…åŠ©å·¥å…·æ¨¡å—ï¼Œä¸åŒ…å«æ ¸å¿ƒé¢„æµ‹ç½‘ç»œï¼Œè€Œæ˜¯ä¸ºæ¨¡å‹è®­ç»ƒå’Œæ•°æ®å¤„ç†æä¾›æ”¯æŒã€‚**



## masked_attention.py

ä½ è´´çš„è¿™æ®µä»£ç æ˜¯ **DUET çš„æ ¸å¿ƒç¼–ç å™¨æ¨¡å—**ï¼ŒåŒ…å«äº†å¤šå¤´æ³¨æ„åŠ›ã€å‚…é‡Œå¶åŸŸçš„ Mahalanobis æ©ç ç­‰åŠŸèƒ½ã€‚æˆ‘å¸®ä½ æŒ‰æ¨¡å—é€ä¸€è®²è§£å®ƒçš„å¤„ç†æµç¨‹å’Œä½œç”¨ï¼š

------

### 1ï¸âƒ£ `EncoderLayer`

```
class EncoderLayer(nn.Module):
    def __init__(self, attention, d_model, d_ff=None, dropout=0.1, activation="relu"):
        ...
```

- å•å±‚ Transformer ç¼–ç å™¨ã€‚
- **è¾“å…¥**: `[B, L, D]`ï¼ˆæ‰¹æ¬¡ã€æ—¶é—´æ­¥ã€ç‰¹å¾ç»´åº¦ï¼‰ã€‚
- **ç»“æ„**:
  1. å¤šå¤´æ³¨æ„åŠ›ï¼ˆ`self.attention`ï¼‰
  2. æ®‹å·® + LayerNorm
  3. å‰é¦ˆå·ç§¯ï¼ˆ`conv1` -> æ¿€æ´» -> `conv2`ï¼‰
  4. ç¬¬äºŒæ¬¡æ®‹å·® + LayerNorm
- **ç‰¹ç‚¹**: ä½¿ç”¨ 1D å·ç§¯ä»£æ›¿æ ‡å‡† FFNï¼Œå…¨å±€æ•æ‰åºåˆ—ç‰¹å¾ã€‚

------

### 2ï¸âƒ£ `Encoder`

```
class Encoder(nn.Module):
    def __init__(self, attn_layers, conv_layers=None, norm_layer=None):
        ...
```

- å°†å¤šå±‚ `EncoderLayer` å †å èµ·æ¥ã€‚
- å¯é€‰å·ç§¯å±‚ï¼ˆ`conv_layers`ï¼‰åœ¨æ¯ä¸ªæ³¨æ„åŠ›å±‚ä¹‹åè¿›ä¸€æ­¥å¤„ç†ã€‚
- **è¾“å‡º**: ç¼–ç åçš„åºåˆ—å’Œæ¯å±‚æ³¨æ„åŠ›æƒé‡ã€‚

------

### 3ï¸âƒ£ `FullAttention`

```
class FullAttention(nn.Module):
    def forward(self, queries, keys, values, attn_mask, tau=None, delta=None):
        ...
```

- æ ‡å‡†ç‚¹ç§¯æ³¨æ„åŠ›ï¼š
  $$
  \text{Attention}(Q,K,V) = \text{softmax}\Big(\frac{QK^T}{\sqrt{d}}\Big) V
  $$

- æ”¯æŒ **masking**ï¼Œå¯ç”¨äº causal maskï¼ˆé¿å…æœªæ¥ä¿¡æ¯æ³„éœ²ï¼‰ã€‚

- å¯é€‰æ‹©è¾“å‡ºæ³¨æ„åŠ›æƒé‡ï¼ˆ`output_attention`ï¼‰ã€‚

------

### 4ï¸âƒ£ `AttentionLayer`

```
class AttentionLayer(nn.Module):
    def __init__(self, attention, d_model, n_heads, d_keys=None, d_values=None):
        ...
```

- å¤šå¤´æ³¨æ„åŠ›å°è£…ï¼š
  1. `query_projection`ã€`key_projection`ã€`value_projection` åˆ†å¤´æ˜ å°„ã€‚
  2. è°ƒç”¨å†…éƒ¨æ³¨æ„åŠ›ï¼ˆ`FullAttention` æˆ–è‡ªå®šä¹‰ attentionï¼‰ã€‚
  3. æ‹¼æ¥è¾“å‡ºå†é€šè¿‡ `out_projection` æ˜ å°„å›åŸç»´åº¦ã€‚
- æ”¯æŒé¢å¤–å‚æ•° `tau`ã€`delta` ç”¨äº DUET çš„æ—¶é—´åŸŸ/é¢‘åŸŸå¤„ç†ã€‚

------

### 5ï¸âƒ£ `Mahalanobis_mask`

```
class Mahalanobis_mask(nn.Module):
    ...
```

- **ç›®çš„**: åœ¨é¢‘åŸŸä¸­å­¦ä¹ é€šé“é—´ç›¸å…³æ€§ï¼Œç”¨äºè‡ªé€‚åº”æ©ç ç”Ÿæˆã€‚
- **æ­¥éª¤**:
  1. å¯¹è¾“å…¥åš **FFT** å˜æ¢ã€‚
  2. è®¡ç®—é€šé“é—´å·®å¼‚ï¼ˆ`diff`ï¼‰ã€‚
  3. Mahalanobis è·ç¦»è®¡ç®—æ¦‚ç‡çŸ©é˜µã€‚
  4. ç”¨ Gumbel-Softmax å¯¹æ¦‚ç‡çŸ©é˜µé‡‡æ ·äºŒå€¼æ©ç ã€‚
- **è¾“å‡º**: `[B, 1, C, C]` çš„æ©ç ï¼Œç”¨äºæ³¨æ„åŠ›å±‚è¿‡æ»¤ä¸ç›¸å…³é€šé“ã€‚

------

### âš¡ DUET ç¼–ç å™¨çš„æ•´ä½“æµç¨‹ï¼ˆç®€åŒ–ç‰ˆï¼‰

1. **è¾“å…¥**: `[B, L, D]` æ—¶é—´åºåˆ—
2. **FFT + Mahalanobis æ©ç **: ç”Ÿæˆé€šé“é—´ç›¸å…³æ€§æ©ç 
3. **å¤šå±‚ç¼–ç å™¨**:
   - æ³¨æ„åŠ›å±‚ + æ®‹å·® + å‰é¦ˆå·ç§¯
   - å¯ç”¨æ©ç è¿‡æ»¤ä¸ç›¸å…³é€šé“
4. **è¾“å‡º**: ç¼–ç åçš„åºåˆ— `[B, L, D]` + æ³¨æ„åŠ›æƒé‡

------

ğŸ”¹ æ ¸å¿ƒè®ºæ–‡å¯¹åº”

è¿™éƒ¨åˆ†ä»£ç æ­£æ˜¯å¯¹åº” **DUET: Dual-Channel Encoder-Transformer for Long-term Time Series Forecasting** è®ºæ–‡ä¸­çš„ **ç¼–ç å™¨ + é€šé“æ©ç æœºåˆ¶**ï¼š

- EncoderLayer & AttentionLayer â†’ è®ºæ–‡ä¸­çš„ â€œDual-Channel Transformer Encoderâ€
- Mahalanobis_mask â†’ è®ºæ–‡ä¸­ç”¨äº **è‡ªé€‚åº”é€šé“é€‰æ‹©** çš„æ¨¡å—