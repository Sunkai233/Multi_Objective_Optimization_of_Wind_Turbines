#  ç½‘ç»œç»„ä»¶ä»£ç åº“è®²è§£ï¼š

[DUET](https://github.com/decisionintelligence/DUET/tree/main)/[ts_benchmark](https://github.com/decisionintelligence/DUET/tree/main/ts_benchmark)/[baselines](https://github.com/decisionintelligence/DUET/tree/main/ts_benchmark/baselines)/[duet](https://github.com/decisionintelligence/DUET/tree/main/ts_benchmark/baselines/duet)/layers/

## 1 Autoformer_EncDec.py



è¿™ä»½ä»£ç æ˜¯ **Autoformer çš„ç¼–ç å™¨å’Œè§£ç å™¨ç»“æ„**ï¼Œ

æ ¸å¿ƒæ€æƒ³æ˜¯æŠŠæ—¶é—´åºåˆ—æ‹†æˆ**è¶‹åŠ¿ï¼ˆtrendï¼‰å’Œå­£èŠ‚æ€§ï¼ˆseasonalï¼‰**ï¼Œå†ç»“åˆæ³¨æ„åŠ›ï¼ˆattentionï¼‰æœºåˆ¶åšé¢„æµ‹ã€‚

ğŸ“Œ å¯¹åº”è®ºæ–‡ï¼š

- **Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting**
   *AAAI 2022, Haixu Wu et al.*

è¿™ç¯‡è®ºæ–‡æå‡ºäº†ï¼š

- **åºåˆ—åˆ†è§£æ¨¡å—ï¼ˆseries decompositionï¼‰**ï¼šæŠŠåºåˆ—æ‹†æˆè¶‹åŠ¿ + å­£èŠ‚æ€§ï¼›
- **æ¸è¿›åˆ†è§£çš„ Encoder/Decoder**ï¼šåœ¨æ³¨æ„åŠ›å’Œå‰é¦ˆç½‘ç»œä¹‹é—´ä¸æ–­å»è¶‹åŠ¿ï¼›
- **Auto-Correlation æœºåˆ¶**ï¼ˆæ¯”æ™®é€š self-attention æ›´é€‚åˆæ—¶é—´åºåˆ—ï¼‰ã€‚

------

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
```

å¼•å…¥ **PyTorch æ¨¡å—**ï¼Œç”¨äºæ„å»ºç¥ç»ç½‘ç»œã€‚

------

### 1. ç‰¹æ®Šçš„ LayerNorm

```python
class my_Layernorm(nn.Module):
    """
    ç‰¹æ®Šçš„ LayerNormï¼Œç”¨äºå­£èŠ‚æ€§éƒ¨åˆ†çš„å½’ä¸€åŒ–ã€‚
    """
    def __init__(self, channels):
        super(my_Layernorm, self).__init__()
        self.layernorm = nn.LayerNorm(channels)

    def forward(self, x):
        x_hat = self.layernorm(x)  # æ™®é€šçš„ LayerNorm
        # åœ¨æ—¶é—´ç»´åº¦ä¸Šå–å‡å€¼ï¼Œä½œä¸º bias
        bias = torch.mean(x_hat, dim=1).unsqueeze(1).repeat(1, x.shape[1], 1)
        return x_hat - bias
```

ğŸ” **è§£é‡Š**ï¼š
 LayerNorm ä¼šå¯¹ç‰¹å¾è¿›è¡Œå½’ä¸€åŒ–ï¼Œè¿™é‡Œé¢å¤–å‡å»æ—¶é—´ç»´åº¦ä¸Šçš„å‡å€¼ï¼Œç¡®ä¿**å­£èŠ‚æ€§æˆåˆ†å›´ç»• 0 æ³¢åŠ¨**ï¼Œçªå‡ºå‘¨æœŸæ€§ã€‚

------

### 2. ç§»åŠ¨å¹³å‡ï¼ˆè¶‹åŠ¿æå–ï¼‰

```python
class moving_avg(nn.Module):
    """
    ç§»åŠ¨å¹³å‡ï¼Œç”¨äºæå–æ—¶é—´åºåˆ—çš„è¶‹åŠ¿éƒ¨åˆ†ã€‚
    """
    def __init__(self, kernel_size, stride):
        super(moving_avg, self).__init__()
        self.kernel_size = kernel_size
        # ä¸€ç»´å¹³å‡æ± åŒ–ï¼Œç›¸å½“äºæ»‘åŠ¨çª—å£å¹³å‡
        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)

    def forward(self, x):
        # åœ¨åºåˆ—ä¸¤ç«¯è¡¥å€¼ï¼Œé˜²æ­¢è¾¹ç•Œä¿¡æ¯ä¸¢å¤±
        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        x = torch.cat([front, x, end], dim=1)
        # æ± åŒ–æ“ä½œï¼ˆæ—¶é—´ç»´åº¦åœ¨ä¸­é—´ï¼Œéœ€è¦ permute è°ƒæ•´ç»´åº¦ï¼‰
        x = self.avg(x.permute(0, 2, 1))
        x = x.permute(0, 2, 1)
        return x
```

ğŸ” **è§£é‡Š**ï¼š
 è¾“å…¥ `x` æ˜¯å½¢çŠ¶ `[batch, time, channels]`ã€‚
 ç§»åŠ¨å¹³å‡ä¼šå¾—åˆ°**å¹³æ»‘è¶‹åŠ¿æ›²çº¿**ã€‚

------

### 3. åºåˆ—åˆ†è§£ï¼ˆåˆ†æˆè¶‹åŠ¿ + æ®‹å·®ï¼‰

```python
class series_decomp(nn.Module):
    """
    æ—¶é—´åºåˆ—åˆ†è§£ï¼šåŸåºåˆ— = æ®‹å·®(å­£èŠ‚æ€§) + è¶‹åŠ¿
    """
    def __init__(self, kernel_size):
        super(series_decomp, self).__init__()
        self.moving_avg = moving_avg(kernel_size, stride=1)

    def forward(self, x):
        moving_mean = self.moving_avg(x)  # æå–è¶‹åŠ¿
        res = x - moving_mean             # æ®‹å·® = å­£èŠ‚æ€§
        return res, moving_mean
```

------

### 4. å¤šå°ºåº¦åˆ†è§£ï¼ˆFEDformer æå‡ºçš„æ‰©å±•ï¼‰

```python
class series_decomp_multi(nn.Module):
    """
    å¤šå°ºåº¦åˆ†è§£ï¼šç”¨ä¸åŒçª—å£å¤§å°çš„ç§»åŠ¨å¹³å‡æ¥æ•æ‰ä¸åŒæ—¶é—´å°ºåº¦çš„è¶‹åŠ¿ã€‚
    """
    def __init__(self, kernel_size):
        super(series_decomp_multi, self).__init__()
        self.kernel_size = kernel_size
        self.series_decomp = [series_decomp(kernel) for kernel in kernel_size]

    def forward(self, x):
        moving_mean = []
        res = []
        for func in self.series_decomp:
            sea, moving_avg = func(x)
            moving_mean.append(moving_avg)
            res.append(sea)

        # å¤šä¸ªåˆ†è§£ç»“æœå–å¹³å‡ï¼Œå¾—åˆ°æ›´ç¨³å®šçš„è¶‹åŠ¿å’Œå­£èŠ‚æ€§
        sea = sum(res) / len(res)
        moving_mean = sum(moving_mean) / len(moving_mean)
        return sea, moving_mean
```

------

### 5. ç¼–ç å™¨å±‚ï¼ˆEncoderLayerï¼‰

```python
class EncoderLayer(nn.Module):
    """
    Autoformer ç¼–ç å™¨å±‚ï¼š
    1. è‡ªæ³¨æ„åŠ›
    2. æ®‹å·® + åˆ†è§£ï¼ˆå»æ‰è¶‹åŠ¿ï¼Œä¿ç•™å­£èŠ‚æ€§ï¼‰
    3. å‰é¦ˆç½‘ç»œï¼ˆä¸¤å±‚å·ç§¯ï¼‰
    4. å†åˆ†è§£
    """
    def __init__(self, attention, d_model, d_ff=None, moving_avg=25, dropout=0.1, activation="relu"):
        super(EncoderLayer, self).__init__()
        d_ff = d_ff or 4 * d_model
        self.attention = attention
        # å‰é¦ˆå±‚ç”¨ 1x1 å·ç§¯å®ç°
        self.conv1 = nn.Conv1d(in_channels=d_model, out_channels=d_ff, kernel_size=1, bias=False)
        self.conv2 = nn.Conv1d(in_channels=d_ff, out_channels=d_model, kernel_size=1, bias=False)
        # ä¸¤æ¬¡åºåˆ—åˆ†è§£
        self.decomp1 = series_decomp(moving_avg)
        self.decomp2 = series_decomp(moving_avg)
        self.dropout = nn.Dropout(dropout)
        self.activation = F.relu if activation == "relu" else F.gelu

    def forward(self, x, attn_mask=None):
        # (1) è‡ªæ³¨æ„åŠ›
        new_x, attn = self.attention(x, x, x, attn_mask=attn_mask)
        x = x + self.dropout(new_x)
        # (2) ç¬¬ä¸€æ¬¡åˆ†è§£ï¼šå»æ‰è¶‹åŠ¿
        x, _ = self.decomp1(x)
        # (3) å‰é¦ˆç½‘ç»œ
        y = self.dropout(self.activation(self.conv1(x.transpose(-1, 1))))
        y = self.dropout(self.conv2(y).transpose(-1, 1))
        # (4) ç¬¬äºŒæ¬¡åˆ†è§£
        res, _ = self.decomp2(x + y)
        return res, attn
```

------

### 6. ç¼–ç å™¨ï¼ˆEncoderï¼‰

```python
class Encoder(nn.Module):
    """
    Autoformer ç¼–ç å™¨ï¼šå¤šä¸ª EncoderLayer å †å ã€‚
    """
    def __init__(self, attn_layers, conv_layers=None, norm_layer=None):
        super(Encoder, self).__init__()
        self.attn_layers = nn.ModuleList(attn_layers)
        self.conv_layers = nn.ModuleList(conv_layers) if conv_layers is not None else None
        self.norm = norm_layer

    def forward(self, x, attn_mask=None):
        attns = []
        if self.conv_layers is not None:
            for attn_layer, conv_layer in zip(self.attn_layers, self.conv_layers):
                x, attn = attn_layer(x, attn_mask=attn_mask)
                x = conv_layer(x)
                attns.append(attn)
            # æœ€åä¸€å±‚åªç”¨æ³¨æ„åŠ›å±‚
            x, attn = self.attn_layers[-1](x)
            attns.append(attn)
        else:
            for attn_layer in self.attn_layers:
                x, attn = attn_layer(x, attn_mask=attn_mask)
                attns.append(attn)

        if self.norm is not None:
            x = self.norm(x)

        return x, attns
```

------

### 7. è§£ç å™¨å±‚ï¼ˆDecoderLayerï¼‰

```python
class DecoderLayer(nn.Module):
    """
    Autoformer è§£ç å™¨å±‚ï¼š
    1. è‡ªæ³¨æ„åŠ›
    2. äº¤å‰æ³¨æ„åŠ›ï¼ˆç¼–ç å™¨ä¿¡æ¯ï¼‰
    3. å‰é¦ˆç½‘ç»œ
    4. å¤šæ¬¡åˆ†è§£ï¼šæå–æ®‹å·® + ç´¯åŠ è¶‹åŠ¿
    """
    def __init__(self, self_attention, cross_attention, d_model, c_out, d_ff=None,
                 moving_avg=25, dropout=0.1, activation="relu"):
        super(DecoderLayer, self).__init__()
        d_ff = d_ff or 4 * d_model
        self.self_attention = self_attention
        self.cross_attention = cross_attention
        self.conv1 = nn.Conv1d(in_channels=d_model, out_channels=d_ff, kernel_size=1, bias=False)
        self.conv2 = nn.Conv1d(in_channels=d_ff, out_channels=d_model, kernel_size=1, bias=False)
        # ä¸‰æ¬¡åˆ†è§£
        self.decomp1 = series_decomp(moving_avg)
        self.decomp2 = series_decomp(moving_avg)
        self.decomp3 = series_decomp(moving_avg)
        self.dropout = nn.Dropout(dropout)
        # æŠ•å½±å±‚ï¼Œç”¨äºè¾“å‡ºè¶‹åŠ¿éƒ¨åˆ†
        self.projection = nn.Conv1d(in_channels=d_model, out_channels=c_out, 
                                    kernel_size=3, stride=1, padding=1,
                                    padding_mode='circular', bias=False)
        self.activation = F.relu if activation == "relu" else F.gelu

    def forward(self, x, cross, x_mask=None, cross_mask=None):
        # (1) è‡ªæ³¨æ„åŠ›
        x = x + self.dropout(self.self_attention(x, x, x, attn_mask=x_mask)[0])
        x, trend1 = self.decomp1(x)
        # (2) äº¤å‰æ³¨æ„åŠ›ï¼ˆåˆ©ç”¨ Encoder çš„è¾“å‡º crossï¼‰
        x = x + self.dropout(self.cross_attention(x, cross, cross, attn_mask=cross_mask)[0])
        x, trend2 = self.decomp2(x)
        # (3) å‰é¦ˆç½‘ç»œ
        y = self.dropout(self.activation(self.conv1(x.transpose(-1, 1))))
        y = self.dropout(self.conv2(y).transpose(-1, 1))
        x, trend3 = self.decomp3(x + y)

        # (4) è¶‹åŠ¿ç´¯åŠ ï¼Œå¹¶ç”¨å·ç§¯æŠ•å½±åˆ°è¾“å‡ºç»´åº¦
        residual_trend = trend1 + trend2 + trend3
        residual_trend = self.projection(residual_trend.permute(0, 2, 1)).transpose(1, 2)
        return x, residual_trend
```

------

### 8. è§£ç å™¨ï¼ˆDecoderï¼‰

```python
class Decoder(nn.Module):
    """
    Autoformer è§£ç å™¨ï¼šå¤šä¸ª DecoderLayer å †å ã€‚
    """
    def __init__(self, layers, norm_layer=None, projection=None):
        super(Decoder, self).__init__()
        self.layers = nn.ModuleList(layers)
        self.norm = norm_layer
        self.projection = projection

    def forward(self, x, cross, x_mask=None, cross_mask=None, trend=None):
        for layer in self.layers:
            x, residual_trend = layer(x, cross, x_mask=x_mask, cross_mask=cross_mask)
            trend = trend + residual_trend

        if self.norm is not None:
            x = self.norm(x)

        if self.projection is not None:
            x = self.projection(x)
        return x, trend
```

- **è¶‹åŠ¿æå– (moving_avg, series_decomp)**ï¼šå¯¹åº”çŠ¶æ€æ–¹ç¨‹ä¸­çš„ç¼“æ…¢å˜åŒ–ï¼ˆä½é¢‘éƒ¨åˆ†ï¼‰ï¼Œç±»ä¼¼é•¿æœŸé£é€Ÿ/è´Ÿè½½è¶‹åŠ¿ï¼›
- **æ®‹å·®/å­£èŠ‚æ€§**ï¼šå¯¹åº”å¿«é€Ÿæ³¢åŠ¨ï¼ˆé«˜é¢‘éƒ¨åˆ†ï¼‰ï¼Œå¦‚åŠŸç‡æŒ¯è¡ï¼›
- **Encoder/Decoder æ³¨æ„åŠ›æœºåˆ¶**ï¼šå»ºæ¨¡ä¸åŒæ—¶é—´ã€å˜é‡é—´çš„ä¾èµ–å…³ç³»ï¼›
- **Decoder è¶‹åŠ¿é¢„æµ‹**ï¼šé€šè¿‡æ®‹å·®+è¶‹åŠ¿å åŠ ï¼Œå¾—åˆ°æœªæ¥çš„é¢„æµ‹ã€‚



éå¸¸å¥½çš„é—®é¢˜ ğŸ‘ æˆ‘æ¥ç»“åˆä½ ç»™çš„è¾“å…¥å½¢çŠ¶ **[batch=32, seq_len=96, feature_dim=7]**ï¼Œèµ°ä¸€é **Autoformer çš„å¤„ç†æµç¨‹**ï¼Œå¹¶ç»“åˆç¼–ç å™¨ã€è§£ç å™¨çš„é€»è¾‘è§£é‡Šå®ƒæ˜¯å¦‚ä½•é¢„æµ‹æœªæ¥ 24 æ­¥çš„ã€‚

------

### ğŸš€ è¾“å…¥è¾“å‡ºè®¾å®š

- **è¾“å…¥åºåˆ—**ï¼š

  XâˆˆR32Ã—96Ã—7X \in \mathbb{R}^{32 \times 96 \times 7}

  ä»£è¡¨ 32 ä¸ªæ ·æœ¬ï¼Œæ¯ä¸ªæ ·æœ¬é•¿åº¦ 96ï¼Œæ¯ä¸ªæ—¶é—´æ­¥æœ‰ 7 ä¸ªç‰¹å¾ï¼ˆä¾‹å¦‚ï¼šé£é€Ÿã€åŠŸç‡ã€å¯†åº¦ç­‰ï¼‰ã€‚

- **é¢„æµ‹è¾“å‡º**ï¼š

  Y^âˆˆR32Ã—24Ã—1\hat{Y} \in \mathbb{R}^{32 \times 24 \times 1}

  é¢„æµ‹æœªæ¥ 24 æ­¥çš„ç›®æ ‡å˜é‡ï¼ˆé€šå¸¸æ˜¯åŠŸç‡/è´Ÿè½½ï¼Œç»´åº¦å¯ä»¥æ˜¯ 1ï¼‰ã€‚

------

### ğŸ” ç¼–ç å™¨ï¼ˆEncoderï¼‰å¤„ç†è¿‡ç¨‹

1. **è¾“å…¥çº¿æ€§æŠ•å½±**ï¼ˆè®ºæ–‡é‡Œæœ‰ embeddingï¼Œè¿™é‡Œçœç•¥ç»†èŠ‚ï¼‰ï¼š
    æŠŠè¾“å…¥çš„ **7 ç»´ç‰¹å¾**æŠ•å½±åˆ° `d_model` ç»´ï¼Œæ¯”å¦‚ `d_model=512`ï¼Œå˜æˆ

   XâˆˆR32Ã—96Ã—512X \in \mathbb{R}^{32 \times 96 \times 512}

2. **ç¬¬ä¸€å±‚ EncoderLayer**ï¼š

   - **è‡ªæ³¨æ„åŠ›**ï¼šå»ºæ¨¡åºåˆ—å†…éƒ¨ 96 ä¸ªæ—¶é—´ç‚¹ä¹‹é—´çš„ä¾èµ–å…³ç³»ï¼›
   - **åˆ†è§£**ï¼šå¾—åˆ° `æ®‹å·®ï¼ˆå­£èŠ‚æ€§éƒ¨åˆ†ï¼‰ + è¶‹åŠ¿`ï¼›
   - **å‰é¦ˆå·ç§¯**ï¼šè¿›ä¸€æ­¥æå–éçº¿æ€§ç‰¹å¾ï¼›
   - **å†æ¬¡åˆ†è§£**ï¼šå»æ‰è¶‹åŠ¿ï¼Œåªç•™ä¸‹æ›´æ–°åçš„æ®‹å·®ã€‚

   è¾“å‡ºä»ç„¶æ˜¯ï¼š

   XâˆˆR32Ã—96Ã—512X \in \mathbb{R}^{32 \times 96 \times 512}

3. **å¤šå±‚å †å **ï¼ˆä¾‹å¦‚ 2~3 å±‚ï¼‰ï¼š
    æ¯ä¸€å±‚éƒ½ä¼šä¸æ–­**å»è¶‹åŠ¿**ï¼Œä¿ç•™ä¸»è¦çš„**å­£èŠ‚æ€§æ³¢åŠ¨**ï¼Œè¶‹åŠ¿éƒ¨åˆ†åˆ™è¢«ä¼ é€’åˆ°è§£ç å™¨ã€‚

------

### ğŸ” è§£ç å™¨ï¼ˆDecoderï¼‰å¤„ç†è¿‡ç¨‹

è¾“å…¥ç»™è§£ç å™¨çš„æœ‰ä¸¤éƒ¨åˆ†ï¼š

1. **Decoder è¾“å…¥åºåˆ—**ï¼š
    é€šå¸¸æ˜¯ç”± `label_len`ï¼ˆå†å²ä¸€éƒ¨åˆ†ï¼Œæ¯”å¦‚ 48 æ­¥ï¼‰+ æœªæ¥çš„ 24 ä¸ª **å ä½ç¬¦ 0** ç»„æˆï¼Œå½¢çŠ¶ï¼š

   DecInputâˆˆR32Ã—(48+24)Ã—512\text{DecInput} \in \mathbb{R}^{32 \times (48+24) \times 512}

   å…¶ä¸­æœªæ¥ 24 æ­¥æ²¡æœ‰çœŸå®å€¼ï¼Œæ‰€ä»¥ç”¨ 0 æˆ–è€… mask ä»£æ›¿ã€‚

2. **Encoder è¾“å‡º**ï¼š

   EncOutputâˆˆR32Ã—96Ã—512\text{EncOutput} \in \mathbb{R}^{32 \times 96 \times 512}

------

### è§£ç å™¨é€å±‚è®¡ç®—

1. **è‡ªæ³¨æ„åŠ›**ï¼ˆDecoder å†…éƒ¨ï¼‰ï¼š
    åªåœ¨ `48+24` åºåˆ—å†…éƒ¨å»ºæ¨¡ï¼Œä¿è¯é¢„æµ‹ç¬¬ t æ­¥æ—¶ä¸ä¼šçœ‹åˆ°æœªæ¥ä¿¡æ¯ã€‚

2. **äº¤å‰æ³¨æ„åŠ›**ï¼š
    å°† `DecInput` ä¸ `EncOutput` å¯¹é½ï¼Œè®©è§£ç å™¨èƒ½â€œæŸ¥è¯¢â€ç¼–ç å™¨é‡Œçš„å†å²æ¨¡å¼ã€‚

3. **åˆ†è§£ä¸‰æ¬¡**ï¼š

   - ç¬¬ä¸€æ¬¡ï¼šå¾—åˆ°å­£èŠ‚æ€§éƒ¨åˆ† + è¶‹åŠ¿1ï¼›
   - ç¬¬äºŒæ¬¡ï¼šç»“åˆäº¤å‰æ³¨æ„åŠ›ç»“æœï¼Œå†åˆ†è§£ï¼Œå¾—åˆ°è¶‹åŠ¿2ï¼›
   - ç¬¬ä¸‰æ¬¡ï¼šç»“åˆå‰é¦ˆç½‘ç»œï¼Œå†åˆ†è§£ï¼Œå¾—åˆ°è¶‹åŠ¿3ã€‚

   æœ€ç»ˆè¶‹åŠ¿ï¼š

   Trend=Trend1+Trend2+Trend3\text{Trend} = \text{Trend}_1 + \text{Trend}_2 + \text{Trend}_3

   å­£èŠ‚æ€§æ®‹å·®ä¼šç»§ç»­ä¼ ä¸‹å»ã€‚

------

### ğŸ“Œ è¾“å‡ºæ‹¼æ¥

1. **æ®‹å·®ï¼ˆå­£èŠ‚æ€§éƒ¨åˆ†ï¼‰**ï¼šä» Decoder æœ€åä¸€å±‚å¾—åˆ°

   SeasonalPartâˆˆR32Ã—24Ã—1\text{SeasonalPart} \in \mathbb{R}^{32 \times 24 \times 1}

2. **è¶‹åŠ¿ï¼ˆTrend éƒ¨åˆ†ï¼‰**ï¼šç”± `projection` å·ç§¯è¾“å‡ºï¼Œå½¢çŠ¶ç›¸åŒ

   TrendPartâˆˆR32Ã—24Ã—1\text{TrendPart} \in \mathbb{R}^{32 \times 24 \times 1}

3. **æœ€ç»ˆé¢„æµ‹**ï¼š

   Y^=SeasonalPart+TrendPart\hat{Y} = \text{SeasonalPart} + \text{TrendPart}

------

### ğŸ¯ ä¸¾ä¸ªç›´è§‚ä¾‹å­

å‡è®¾æˆ‘æœ‰ **é£é€Ÿã€é£å‘ã€æ°”æ¸©ã€æ°”å‹ã€ç©ºæ°”å¯†åº¦ã€æ¡¨è·è§’ã€è½¬é€Ÿ** è¿™ 7 ä¸ªç‰¹å¾ï¼Œè¾“å…¥ **96 æ­¥ï¼ˆ4 å¤©ï¼Œæ¯å¤© 24 å°æ—¶ï¼‰**ï¼Œæˆ‘è¦é¢„æµ‹æœªæ¥ **24 æ­¥ï¼ˆ1 å¤©ï¼‰**çš„åŠŸç‡ï¼š

1. **Encoder**ï¼š
   - æŠŠå†å² 96 æ­¥åˆ†è§£æˆ**é•¿æœŸè¶‹åŠ¿ï¼ˆé£é€Ÿé€æ¸å¢å¼ºï¼‰** + **å‘¨æœŸæ€§æ³¢åŠ¨ï¼ˆç™½å¤©/å¤œæ™šæ¸©åº¦å‘¨æœŸã€è½¬é€Ÿå‘¨æœŸï¼‰**ï¼›
   - ä¿ç•™å‘¨æœŸæ€§æ®‹å·®ï¼Œä¸¢æ‰è¶‹åŠ¿éƒ¨åˆ†ï¼ˆä½†è¶‹åŠ¿ä¼šé€åˆ°è§£ç å™¨ï¼‰ã€‚
2. **Decoder**ï¼š
   - è¾“å…¥ = æœ€è¿‘ 48 æ­¥çœŸå®å€¼ + æœªæ¥ 24 æ­¥å ä½ç¬¦ï¼›
   - è‡ªæ³¨æ„åŠ›å­¦åˆ°â€œè¿‡å» 48 æ­¥çš„å‘¨æœŸç»“æ„â€ï¼›
   - äº¤å‰æ³¨æ„åŠ›æŸ¥è¯¢ç¼–ç å™¨ï¼Œè·å¾—å®Œæ•´ 96 æ­¥å†å²çš„è¶‹åŠ¿ï¼›
   - åˆ†è§£å¹¶é€æ­¥åŠ ä¸Šè¶‹åŠ¿ï¼Œé¢„æµ‹æœªæ¥ 24 æ­¥åŠŸç‡ã€‚
3. **è¾“å‡º**ï¼š
   - æœªæ¥ 24 æ­¥çš„åŠŸç‡é¢„æµ‹æ›²çº¿ï¼Œä¼šåŒ…å«ï¼š
     - **è¶‹åŠ¿**ï¼šæ¯”å¦‚æœªæ¥ä¸€å¤©é£é€ŸæŒç»­å‡é«˜ï¼ŒåŠŸç‡é€æ¸å¢å¤§ï¼›
     - **å­£èŠ‚æ€§**ï¼šä¾‹å¦‚ 24 å°æ—¶çš„æ˜¼å¤œæ³¢åŠ¨ã€å‘¨æœŸæ€§è´Ÿè½½å˜åŒ–ã€‚

------

è¦ä¸è¦æˆ‘å¸®ä½ ç”»ä¸€å¼  **æ•°æ®æµå‘ç¤ºæ„å›¾**ï¼ˆEncoder/Decoder + Trend/Seasonal åˆ†è§£ç®­å¤´ï¼‰ï¼Œè¿™æ ·ä½ å¯ä»¥åœ¨æ±‡æŠ¥é‡Œç›´è§‚å±•ç¤ºã€[32,96,7] â†’ [32,24,1]ã€‘çš„å¤„ç†è¿‡ç¨‹ï¼Ÿ



### 1. è¶‹åŠ¿ vs å­£èŠ‚æ€§

- **è¶‹åŠ¿ï¼ˆtrendï¼‰**ï¼š
   æ˜¯ä½é¢‘ã€å¹³æ»‘çš„æˆåˆ†ï¼ˆé•¿æœŸç¼“æ…¢ä¸Šå‡/ä¸‹é™ï¼‰ã€‚æ¯”å¦‚é£é€Ÿé€æ¸å¢å¼ºã€ç”µä»·æ…¢æ…¢ä¸Šæ¶¨ã€‚
   ğŸ‘‰ é€šå¸¸ **å˜åŒ–æ…¢ï¼Œç»“æ„ç®€å•**ï¼Œç”šè‡³å¯ä»¥ç”¨çº¿æ€§/å·ç§¯æ‹Ÿåˆå‡ºæ¥ã€‚
- **å­£èŠ‚æ€§ï¼ˆseasonalï¼‰/æ®‹å·®**ï¼š
   æ˜¯é«˜é¢‘ã€å¿«é€Ÿæ³¢åŠ¨çš„éƒ¨åˆ†ï¼ˆå‘¨æœŸæŒ¯è¡ã€æ˜¼å¤œå‘¨æœŸã€çªå‘æ³¢åŠ¨ï¼‰ã€‚
   ğŸ‘‰ **å¤æ‚ã€éçº¿æ€§ã€ä¾èµ–æ€§å¼º**ï¼Œéœ€è¦å¼ºæ¨¡å‹ï¼ˆå¦‚æ³¨æ„åŠ›æœºåˆ¶ï¼‰æ¥æ•æ‰ã€‚

------

### 2. Transformer çš„â€œçŸ­æ¿â€

æ™®é€š Transformerï¼ˆæ¯”å¦‚ Informerã€Transformer-TSï¼‰æœ‰ä¸¤ä¸ªé—®é¢˜ï¼š

1. **æ³¨æ„åŠ›å¾ˆå®¹æ˜“å»æ‹Ÿåˆè¶‹åŠ¿**
    è¶‹åŠ¿æ˜¯ä¸€ç§ä½é¢‘ä¿¡å·ï¼Œæ¨¡å‹å¾ˆå®¹æ˜“å°±æŠŠæ³¨æ„åŠ›æ”¾åœ¨â€œåºåˆ—æ•´ä½“å½¢çŠ¶â€ä¸Šï¼Œè€Œä¸æ˜¯ç»†èŠ‚ã€‚è¿™æ ·ä¼šå¯¼è‡´æ¨¡å‹é¢„æµ‹æ—¶**è¾“å‡ºä¸€æ¡å¹³æ»‘æ›²çº¿**ï¼Œä½†ç¼ºå°‘å‘¨æœŸæ€§æ³¢åŠ¨ã€‚
2. **éš¾ä»¥å»ºæ¨¡é•¿åºåˆ—çš„é«˜é¢‘ä¾èµ–**
    å­£èŠ‚æ€§/å‘¨æœŸæ€§æ‰æ˜¯ Transformer åº”è¯¥é‡ç‚¹å­¦çš„ä¸œè¥¿ï¼Œä½†å¦‚æœè¶‹åŠ¿ä¸å»æ‰ï¼Œå®ƒä¼šæ©ç›–æ‰é«˜é¢‘ä¾èµ–ã€‚

------

### 3. Autoformer çš„åšæ³•ï¼šä¸æ–­å»è¶‹åŠ¿

Autoformer çš„æ ¸å¿ƒæ€æƒ³æ˜¯ï¼š

- **è¶‹åŠ¿**ï¼šç®€å• â†’ å•ç‹¬æå–å‡ºæ¥ï¼Œç”¨å·ç§¯ Projection æ¥é¢„æµ‹ã€‚
- **å­£èŠ‚æ€§**ï¼šå¤æ‚ â†’ ç•™åœ¨æ®‹å·®é‡Œäº¤ç»™æ³¨æ„åŠ›æœºåˆ¶å»å»ºæ¨¡ã€‚

è¿™å°±æ˜¯ä¸ºä»€ä¹ˆæ¯ä¸€å±‚ **Encoder/Decoder** ä¹‹é—´éƒ½è¦â€œå»ä¸€æ¬¡è¶‹åŠ¿â€ï¼š

1. **Encoder å±‚**ï¼š
   - è¾“å…¥ = æ®‹å·® + è¶‹åŠ¿
   - å»æ‰è¶‹åŠ¿ï¼Œåªç•™ä¸‹æ®‹å·®ï¼Œé€è¿›ä¸‹ä¸€å±‚ã€‚
   - è¶‹åŠ¿éƒ¨åˆ†åˆ™è¢«ä¸¢å¼ƒï¼ˆå› ä¸ºç®€å•ï¼‰ã€‚
2. **Decoder å±‚**ï¼š
   - æ¯æ¬¡ attention å’Œå‰é¦ˆåéƒ½ä¼šåˆ†è§£ â†’ å¾—åˆ°è¶‹åŠ¿åˆ†é‡ã€‚
   - æ‰€æœ‰è¶‹åŠ¿åˆ†é‡åœ¨æœ€å**ç´¯åŠ èµ·æ¥**ï¼Œå†åŠ ä¸Šæ®‹å·®é¢„æµ‹ï¼Œå½¢æˆæœ€ç»ˆç»“æœã€‚

------

### 4. å¥½å¤„

- **å»ºæ¨¡æ›´èšç„¦**ï¼šæ³¨æ„åŠ›å±‚åªéœ€è¦å…³æ³¨å­£èŠ‚æ€§ï¼ˆå¤æ‚å…³ç³»ï¼‰ï¼Œä¸ä¼šè¢«è¶‹åŠ¿å¹²æ‰°ã€‚
- **æé«˜ç¨³å®šæ€§**ï¼šè¶‹åŠ¿è¢«åˆ†è§£å‡ºæ¥åï¼Œæ¨¡å‹ä¸å®¹æ˜“â€œæ¼‚ç§»â€ã€‚
- **æå‡é•¿æœŸé¢„æµ‹æ•ˆæœ**ï¼šé•¿æœŸé¢„æµ‹é‡Œï¼Œè¶‹åŠ¿éƒ¨åˆ†å®¹æ˜“å¤–æ¨ï¼Œè€Œå­£èŠ‚æ€§éƒ¨åˆ†éœ€è¦æ•æ‰ä¾èµ–å…³ç³» â†’ Autoformer æ­£å¥½æŠŠä¸¤è€…åˆ†å¼€å»ºæ¨¡ã€‚





## 2 linear_pattern_extractor.py

å¥½çš„ ğŸ‘ æˆ‘æ¥é€æ®µè®²è§£è¿™ä»½ **`Linear_extractor`** ä»£ç ï¼Œç»“åˆ Autoformer çš„æ€æƒ³ã€‚

è¿™ä¸ªæ¨¡å—ä¸»è¦åŠŸèƒ½æ˜¯ï¼š
 ğŸ‘‰ æŠŠè¾“å…¥çš„æ—¶é—´åºåˆ— **åˆ†è§£æˆè¶‹åŠ¿ (trend) + å­£èŠ‚æ€§ (seasonal)**ï¼Œç„¶åç”¨ **çº¿æ€§å±‚** å¯¹äºŒè€…åˆ†åˆ«å»ºæ¨¡ï¼Œæœ€åå†åˆæˆé¢„æµ‹ç»“æœã€‚

------

### ä»£ç è§£æ

```python
import torch
import torch.nn as nn
from ..layers.Autoformer_EncDec import series_decomp
```

- å¼•å…¥ PyTorch åŸºç¡€æ¨¡å—ã€‚
- `series_decomp`ï¼šæ˜¯ Autoformer å®šä¹‰çš„ **åºåˆ—åˆ†è§£å‡½æ•°**ï¼ŒæŠŠè¾“å…¥åºåˆ—æ‹†åˆ†ä¸º **å­£èŠ‚æ€§éƒ¨åˆ†** å’Œ **è¶‹åŠ¿éƒ¨åˆ†**ã€‚

------

```python
class Linear_extractor(nn.Module):
    """
    Paper link: https://arxiv.org/pdf/2205.13504.pdf
    è¿™ä¸ªæ¨¡å—æºè‡ª FEDformer/Autoformerï¼Œä¸»è¦æ€æƒ³æ˜¯ï¼š
    - åˆ†è§£åºåˆ— = å­£èŠ‚æ€§ + è¶‹åŠ¿
    - å„è‡ªç”¨çº¿æ€§æ˜ å°„åšé¢„æµ‹
    """
```

------

#### åˆå§‹åŒ–

```python
def __init__(self, configs, individual=False):
    super(Linear_extractor, self).__init__()
    self.seq_len = configs.seq_len     # è¾“å…¥åºåˆ—é•¿åº¦
    self.pred_len = configs.d_model    # è¾“å‡ºåºåˆ—é•¿åº¦ (é¢„æµ‹æ­¥æ•°ï¼Œç”¨ d_model é…ç½®)
    self.decompsition = series_decomp(configs.moving_avg)  # è¶‹åŠ¿ + å­£èŠ‚æ€§åˆ†è§£å™¨
    self.individual = individual       # æ˜¯å¦å¯¹æ¯ä¸ªå˜é‡å•ç‹¬å»ºæ¨¡
    self.channels = configs.enc_in     # è¾“å…¥é€šé“æ•°ï¼ˆå˜é‡æ•°ï¼‰
    self.enc_in = 1 if configs.CI else configs.enc_in  # æ˜¯å¦æ¯ä¸ªå˜é‡ç‹¬ç«‹å¤„ç†
```

- **`configs.seq_len`**ï¼šå†å²çª—å£é•¿åº¦ï¼Œä¾‹å¦‚è¾“å…¥ 96 æ­¥ã€‚
- **`configs.d_model`**ï¼šé¢„æµ‹é•¿åº¦ï¼Œä¾‹å¦‚è¾“å‡ºæœªæ¥ 24 æ­¥ã€‚
- **`configs.enc_in`**ï¼šè¾“å…¥å˜é‡æ•°é‡ï¼ˆå¤šå…ƒæ—¶é—´åºåˆ—ï¼‰ã€‚
- **`configs.CI`**ï¼šé€šé“ç‹¬ç«‹æ¨¡å¼ï¼Œè‹¥å¼€å¯åˆ™æ¯ä¸ªé€šé“å•ç‹¬å¤„ç†ã€‚

------

#### ç‹¬ç«‹ / å…±äº«æ¨¡å¼

```python
if self.individual:
    # æ¯ä¸ªå˜é‡å•ç‹¬ä¸€å¥—çº¿æ€§å±‚
    self.Linear_Seasonal = nn.ModuleList()
    self.Linear_Trend = nn.ModuleList()

    for i in range(self.channels):
        self.Linear_Seasonal.append(nn.Linear(self.seq_len, self.pred_len))
        self.Linear_Trend.append(nn.Linear(self.seq_len, self.pred_len))

        # åˆå§‹åŒ–ï¼šæƒé‡å¹³å‡åˆ†å¸ƒ (å‡åŒ€åŠ æƒ)
        self.Linear_Seasonal[i].weight = nn.Parameter(
            (1 / self.seq_len) * torch.ones([self.pred_len, self.seq_len]))
        self.Linear_Trend[i].weight = nn.Parameter(
            (1 / self.seq_len) * torch.ones([self.pred_len, self.seq_len]))
else:
    # æ‰€æœ‰å˜é‡å…±äº«ä¸€å¥—çº¿æ€§å±‚
    self.Linear_Seasonal = nn.Linear(self.seq_len, self.pred_len)
    self.Linear_Trend = nn.Linear(self.seq_len, self.pred_len)

    self.Linear_Seasonal.weight = nn.Parameter(
        (1 / self.seq_len) * torch.ones([self.pred_len, self.seq_len]))
    self.Linear_Trend.weight = nn.Parameter(
        (1 / self.seq_len) * torch.ones([self.pred_len, self.seq_len]))
```

ğŸ” **è§£é‡Šï¼š**

- **Individual æ¨¡å¼**ï¼šå¯¹æ¯ä¸ªå˜é‡ç‹¬ç«‹å»ºæ¨¡ï¼Œäº’ä¸å¹²æ‰°ã€‚
- **å…±äº«æ¨¡å¼**ï¼šæ‰€æœ‰å˜é‡å…±äº«çº¿æ€§å±‚ï¼Œå‡è®¾å®ƒä»¬æœä»ç›¸åŒçš„è§„å¾‹ã€‚
- **åˆå§‹åŒ–**ï¼šæƒé‡è®¾ç½®ä¸ºå‡åŒ€åˆ†å¸ƒï¼Œæ„å‘³ç€é¢„æµ‹ä¸€å¼€å§‹æ˜¯â€œå¹³å‡å€¼å¤–æ¨â€ï¼Œé¿å…æ¨¡å‹åç½®å¤ªå¤§ã€‚

------

#### ç¼–ç å™¨

```python
def encoder(self, x):
    seasonal_init, trend_init = self.decompsition(x)  # åˆ†è§£æˆå­£èŠ‚æ€§+è¶‹åŠ¿
    seasonal_init, trend_init = seasonal_init.permute(0, 2, 1), trend_init.permute(0, 2, 1)
    # ç°åœ¨ç»´åº¦æ˜¯ [B, D, L]ï¼Œæ–¹ä¾¿çº¿æ€§å±‚æ“ä½œ

    if self.individual:
        seasonal_output = torch.zeros([seasonal_init.size(0), seasonal_init.size(1), self.pred_len],
                                      dtype=seasonal_init.dtype).to(seasonal_init.device)
        trend_output = torch.zeros([trend_init.size(0), trend_init.size(1), self.pred_len],
                                   dtype=trend_init.dtype).to(trend_init.device)

        for i in range(self.channels):
            seasonal_output[:, i, :] = self.Linear_Seasonal[i](seasonal_init[:, i, :])
            trend_output[:, i, :] = self.Linear_Trend[i](trend_init[:, i, :])
    else:
        seasonal_output = self.Linear_Seasonal(seasonal_init)
        trend_output = self.Linear_Trend(trend_init)

    x = seasonal_output + trend_output  # èåˆè¶‹åŠ¿+å­£èŠ‚æ€§
    return x.permute(0, 2, 1)           # è½¬å› [B, L, D]
```

ğŸ” **è§£é‡Šï¼š**

1. è¾“å…¥ `x` å½¢çŠ¶ `[B, L, D]`ã€‚
2. `series_decomp(x)` â†’ åˆ†è§£ä¸º **è¶‹åŠ¿** å’Œ **å­£èŠ‚æ€§**ã€‚
3. æ¯éƒ¨åˆ†ç”¨ **çº¿æ€§å±‚** ä»è¾“å…¥é•¿åº¦ â†’ é¢„æµ‹é•¿åº¦åšæ˜ å°„ã€‚
4. æœ€åæŠŠè¶‹åŠ¿ + å­£èŠ‚æ€§åŠ èµ·æ¥ï¼Œå¾—åˆ°é¢„æµ‹åºåˆ—ã€‚

------

#### é¢„æµ‹å‡½æ•°

```python
def forecast(self, x_enc):
    return self.encoder(x_enc)
```

- ç®€å•å°è£…ï¼Œæ–¹ä¾¿è°ƒç”¨ã€‚

------

#### å‰å‘ä¼ æ’­

```python
def forward(self, x_enc):
    if x_enc.shape[0] == 0:
        return torch.empty((0, self.pred_len, self.enc_in)).to(x_enc.device)
    dec_out = self.forecast(x_enc)
    return dec_out[:, -self.pred_len:, :]  # [B, L, D]
```

ğŸ” **è§£é‡Šï¼š**

- `x_enc`ï¼šè¾“å…¥åºåˆ— `[B, seq_len, D]`ã€‚
- `forecast(x_enc)`ï¼šå¾—åˆ°é¢„æµ‹è¾“å‡º `[B, pred_len, D]`ã€‚
- `dec_out[:, -self.pred_len:, :]`ï¼šå–æœ€å `pred_len` ä¸ªæ—¶é—´æ­¥ä½œä¸ºæœ€ç»ˆé¢„æµ‹ã€‚



è¿™ä¸ª `Linear_extractor` çš„ä½œç”¨æ˜¯ï¼š

1. **åˆ†è§£**è¾“å…¥æ—¶é—´åºåˆ— = è¶‹åŠ¿ + å­£èŠ‚æ€§ï¼›
2. **åˆ†åˆ«å»ºæ¨¡**è¶‹åŠ¿å’Œå­£èŠ‚æ€§ï¼Œç”¨çº¿æ€§å±‚åšæŠ•å½±ï¼›
3. **é‡æ„è¾“å‡º**ä½œä¸ºé¢„æµ‹åºåˆ—ã€‚

å®ƒæœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ª **çº¿æ€§é¢„æµ‹å™¨**ï¼Œå¯ä»¥çœ‹ä½œ Autoformer/FEDformer çš„ã€Œä½é˜¶æ¨¡å—ã€ã€‚
 å¤æ‚æ¨¡å‹ï¼ˆæ¯”å¦‚ DUETï¼‰ä¼šæŠŠå®ƒä½œä¸ºå­æ¨¡å—ï¼Œå†å åŠ æ³¨æ„åŠ›æœºåˆ¶ã€èšç±»æœºåˆ¶æ¥å¢å¼ºã€‚





å¥½ï¼Œæˆ‘æ¥ç»“åˆä½ ç»™çš„ **`Linear_extractor`**ï¼Œç”¨ä¸€ä¸ªå…·ä½“ä¾‹å­ï¼ˆæ¯”å¦‚è¾“å…¥ **[32,96,7]**ï¼Œé¢„æµ‹æœªæ¥ **24 æ­¥**ï¼‰èµ°ä¸€éå®ƒçš„å¤„ç†è¿‡ç¨‹ã€‚è¿™æ ·ä¼šæ›´ç›´è§‚ã€‚

------

### ğŸ¯ å‡è®¾è¾“å…¥

- **è¾“å…¥åºåˆ—**
  $$
  x \in \mathbb{R}^{32 \times 96 \times 7}
  $$

  - batch = 32
  - å†å²é•¿åº¦ = 96ï¼ˆè¿‡å» 96 ä¸ªæ—¶é—´æ­¥ï¼Œæ¯”å¦‚ 4 å¤©ï¼Œæ¯å¤© 24 å°æ—¶ï¼‰
  - ç‰¹å¾æ•° = 7ï¼ˆé£é€Ÿã€æ°”æ¸©ã€æ°”å‹ã€åŠŸç‡ã€æ¡¨è·è§’ã€å¯†åº¦ã€è½¬é€Ÿï¼‰

- **ç›®æ ‡**
   é¢„æµ‹æœªæ¥ 24 æ­¥ï¼ˆ1 å¤©ï¼‰çš„ 7 ä¸ªå˜é‡ï¼š
  $$
  \hat{y} \in \mathbb{R}^{32 \times 24 \times 7}
  $$

------

### ğŸ” Linear_extractor çš„å¤„ç†æµç¨‹

1. **åºåˆ—åˆ†è§£**

```
seasonal_init, trend_init = self.decompsition(x)
```

- `series_decomp` ç”¨æ»‘åŠ¨å¹³å‡åˆ†è§£è¾“å…¥åºåˆ—ï¼š
  $$
  x = \text{seasonal\_init} + \text{trend\_init}
  $$

ä¸¾ä¾‹ï¼š

- **è¶‹åŠ¿**ï¼šé£é€Ÿæ•´ä½“é€æ¸å‡é«˜ï¼ŒåŠŸç‡é€æ¸å¢å¤§ï¼›
- **å­£èŠ‚æ€§**ï¼šæ˜¼å¤œå‘¨æœŸæ³¢åŠ¨ï¼ˆ24 å°æ—¶å‘¨æœŸï¼‰ï¼Œæˆ–è€…çŸ­æœŸæ³¢åŠ¨ã€‚

------

2. **è½¬ç½®ç»´åº¦**

```
seasonal_init, trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1)
```

æŠŠ `[32,96,7]` å˜æˆ `[32,7,96]`ï¼Œè¿™æ · **æ¯ä¸ªå˜é‡å•ç‹¬æ˜¯ä¸€ä¸ªæ—¶é—´åºåˆ—**ï¼Œæ–¹ä¾¿é€è¿› `nn.Linear`ã€‚

------

3. **çº¿æ€§æ˜ å°„é¢„æµ‹**

- å¦‚æœæ˜¯ **individual=True**ï¼šæ¯ä¸ªå˜é‡éƒ½æœ‰ç‹¬ç«‹çš„ `nn.Linear`ï¼ˆä» 96 â†’ 24ï¼‰ã€‚
- å¦‚æœæ˜¯ **å…±äº«æ¨¡å¼**ï¼šæ‰€æœ‰å˜é‡å…±äº«ä¸€å¥— `nn.Linear`ã€‚

```
seasonal_output = self.Linear_Seasonal(seasonal_init)  # [32,7,24]
trend_output    = self.Linear_Trend(trend_init)        # [32,7,24]
```

- `nn.Linear(96,24)` ç­‰ä»·äºå­¦åˆ°ä¸€ä¸ªã€ŒæŠ•å½±çŸ©é˜µã€ï¼ŒæŠŠè¿‡å» 96 æ­¥çº¿æ€§ç»„åˆï¼Œå¾—åˆ°æœªæ¥ 24 æ­¥ã€‚
- åˆå§‹æƒé‡æ˜¯å‡åŒ€åˆ†å¸ƒ â†’ åˆšå¼€å§‹é¢„æµ‹ç›¸å½“äºåšâ€œå¹³å‡å€¼å¤–æ¨â€ã€‚

------

4. **èåˆè¶‹åŠ¿ + å­£èŠ‚æ€§**

```
x = seasonal_output + trend_output
```

å¾—åˆ°ï¼š
$$
\hat{y} \in \mathbb{R}^{32 \times 7 \times 24}
$$
å†è½¬å› `[32,24,7]` æ–¹ä¾¿åç»­ä½¿ç”¨ã€‚

------

5. **è¾“å‡ºé¢„æµ‹**

```
return dec_out[:, -self.pred_len:, :] 
```

- `dec_out` å½¢çŠ¶ `[32,24,7]`
- æœ€ç»ˆè¾“å‡ºå°±æ˜¯æœªæ¥ 24 æ­¥çš„ 7 ä¸ªå˜é‡ã€‚

------

### ğŸ“Œ ä¸¾ä¸ªç›´è§‚ä¾‹å­

å‡è®¾è¾“å…¥å˜é‡æ˜¯ï¼š

- **é£é€Ÿ**ï¼ˆé€æ¸å‡é«˜ + æ˜¼å¤œæ³¢åŠ¨ï¼‰
- **æ°”æ¸©**ï¼ˆä¸‹é™è¶‹åŠ¿ + æ—¥é—´é«˜å¤œé—´ä½ï¼‰
- **åŠŸç‡**ï¼ˆéšé£é€Ÿå˜åŒ– + æŒ¯è¡ï¼‰

é‚£ä¹ˆ `Linear_extractor` çš„å¤„ç†ï¼š

1. **åˆ†è§£é£é€Ÿåºåˆ—**ï¼š
   - è¶‹åŠ¿ï¼šæ•´ä½“å‡é«˜çš„ç›´çº¿ï¼›
   - å­£èŠ‚æ€§ï¼š24 å°æ—¶å‘¨æœŸæ€§æŒ¯è¡ã€‚
2. **çº¿æ€§é¢„æµ‹è¶‹åŠ¿éƒ¨åˆ†**ï¼š
   - è¶‹åŠ¿çº¿è¢«å¤–æ¨ï¼ˆçº¿æ€§å»¶é•¿ï¼‰ï¼Œæœªæ¥ 24 å°æ—¶ç»§ç»­å‡é«˜ã€‚
3. **çº¿æ€§é¢„æµ‹å­£èŠ‚æ€§éƒ¨åˆ†**ï¼š
   - æŠŠè¿‡å»çš„å‘¨æœŸæ¨¡å¼â€œå¤åˆ¶â€åˆ°æœªæ¥ â†’ å¾—åˆ°æ˜¼å¤œæ³¢åŠ¨ã€‚
4. **èåˆ**ï¼š
   - æœªæ¥é£é€Ÿé¢„æµ‹ = å‡é«˜çš„è¶‹åŠ¿ + å‘¨æœŸæ€§çš„æ˜¼å¤œæ³¢åŠ¨ï¼›
   - åŠŸç‡ã€æ°”æ¸©ç­‰å˜é‡ä¹Ÿç±»ä¼¼ã€‚

æœ€ç»ˆè¾“å‡ºï¼š
 æœªæ¥ 24 å°æ—¶çš„ **é£é€Ÿã€æ°”æ¸©ã€åŠŸç‡** ç­‰é¢„æµ‹æ›²çº¿ã€‚



## 3 distributional_router_encoder.py

ğŸ‘Œ æˆ‘æ¥å¸®ä½ è¯¦ç»†è®²è§£è¿™ä¸ª **`encoder`** æ¨¡å—ã€‚

è¿™ä¸ªæ¨¡å—çœ‹èµ·æ¥æ˜¯ **DUET** é‡Œ **distributional router encoder** çš„ä¸€ä¸ªå®ç°ï¼Œç”¨äº **æ—¶é—´ç»´åº¦èšç±» (Temporal Clustering)**ï¼ŒæŠŠè¾“å…¥åºåˆ—æ˜ å°„åˆ°â€œä¸“å®¶ (experts)â€çš„åˆ†å¸ƒä¸Šã€‚

------

### ä»£ç é€æ®µè§£æ

```python
class encoder(nn.Module):
    def __init__(self, config):
        super(encoder, self).__init__()
        input_size = config.seq_len          # è¾“å…¥åºåˆ—é•¿åº¦
        num_experts = config.num_experts     # ä¸“å®¶ä¸ªæ•° (èšç±»æ•°)
        encoder_hidden_size = config.hidden_size  # éšè—å±‚ç»´åº¦

        # ä¸€ä¸ª 2 å±‚ MLPï¼Œç”¨æ¥æ‹Ÿåˆâ€œåˆ†å¸ƒâ€
        self.distribution_fit = nn.Sequential(
            nn.Linear(input_size, encoder_hidden_size, bias=False),  # [B, L] -> [B, H]
            nn.ReLU(),                                              # éçº¿æ€§æ¿€æ´»
            nn.Linear(encoder_hidden_size, num_experts, bias=False) # [B, H] -> [B, num_experts]
        )
```

ğŸ” **è§£é‡Šï¼š**

- è¾“å…¥åºåˆ—é•¿åº¦ä¸º `seq_len`ï¼Œå³æ¯ä¸ªæ—¶é—´åºåˆ—çš„å†å²çª—å£å¤§å°ã€‚
- æ¨¡å—å†…éƒ¨æ˜¯ä¸€ä¸ª **ä¸¤å±‚å…¨è¿æ¥ç½‘ç»œ (MLP)**ï¼ŒæŠŠåºåˆ—ç‰¹å¾å‹ç¼©åˆ° `hidden_size`ï¼Œå†æ˜ å°„åˆ° `num_experts`ã€‚
- è¾“å‡ºçš„ç»´åº¦ç­‰äº `num_experts`ï¼Œå¯ä»¥ç†è§£ä¸ºâ€œè¾“å…¥åºåˆ—å±äºæ¯ä¸ªä¸“å®¶çš„æ¦‚ç‡/æƒé‡â€ã€‚

------

```python
def forward(self, x):
    mean = torch.mean(x, dim=-1)   # åœ¨æœ€åä¸€ä¸ªç»´åº¦ä¸Šå–å¹³å‡
    out = self.distribution_fit(mean)
    return out
```

- **è¾“å…¥ `x`**ï¼šé€šå¸¸å½¢çŠ¶ `[B, L, D]`
  - `B` = batch size
  - `L` = åºåˆ—é•¿åº¦ (`seq_len`)
  - `D` = ç‰¹å¾æ•°ï¼ˆå˜é‡æ•°ï¼Œenc_inï¼‰
- `torch.mean(x, dim=-1)`ï¼šå¯¹å˜é‡ç»´åº¦å–å‡å€¼ï¼Œå¾—åˆ° `[B, L]`ï¼Œç›¸å½“äºæŠŠå¤šä¸ªå˜é‡å‹ç¼©æˆå•é€šé“ã€‚
- `distribution_fit(mean)`ï¼šæŠŠ `[B, L]` æ˜ å°„åˆ° `[B, num_experts]`ï¼Œè¾“å‡ºè¡¨ç¤ºè¯¥åºåˆ—åœ¨ **å„ä¸ªä¸“å®¶/èšç±»** ä¸‹çš„â€œåˆ†å¸ƒâ€ã€‚

------

### ğŸ”§ ä¸¾ä¾‹è¯´æ˜

å‡è®¾ï¼š

- `seq_len=96`ï¼ˆè¾“å…¥ 96 æ­¥æ—¶é—´åºåˆ—ï¼‰
- `num_experts=4`ï¼ˆèšç±»æˆ 4 ç±»æ—¶é—´æ¨¡å¼ï¼‰
- `hidden_size=128`

æµç¨‹ï¼š

1. è¾“å…¥ `x: [32, 96, 7]`ï¼ˆbatch=32ï¼Œæœ‰ 7 ä¸ªå˜é‡ï¼‰ã€‚
2. å¯¹å˜é‡å–å¹³å‡ï¼š`mean: [32, 96]`ã€‚
3. ç»è¿‡ MLPï¼š`out: [32, 4]`ã€‚
   - æ¯ä¸ªæ ·æœ¬éƒ½ä¼šå¾—åˆ°ä¸€ä¸ª `[4]` å‘é‡ï¼Œè¡¨ç¤ºåœ¨ 4 ä¸ªä¸“å®¶ä¸Šçš„æƒé‡ã€‚

------

### ğŸ§© ä½œç”¨

è¿™ä¸ª `encoder` çš„ä½œç”¨æ˜¯ **æŠŠåŸå§‹æ—¶é—´åºåˆ—åˆ†é…åˆ°ä¸åŒçš„ä¸“å®¶ (clusters)**ï¼Œå…·ä½“æ¥è¯´ï¼š

- ç±»ä¼¼äº **soft clustering**ï¼šä¸æ˜¯ç¡¬åˆ†ç±»ï¼Œè€Œæ˜¯å¾—åˆ°ä¸€ä¸ªåˆ†å¸ƒã€‚
- æ¯ä¸ªä¸“å®¶å¯èƒ½ä»£è¡¨ **ä¸åŒæ—¶é—´æ¨¡å¼**ï¼ˆæ¯”å¦‚ï¼šå¹³ç¨³åŒºé—´ã€çªå˜åŒºé—´ã€å‘¨æœŸæ€§æ¨¡å¼ã€è¶‹åŠ¿æ¨¡å¼ï¼‰ã€‚
- DUET å°±æ˜¯ç”¨è¿™ç§æ–¹å¼æ¥ **åˆ†è§£å¤æ‚çš„æ—¶é—´ç»´åº¦å…³ç³»**





## 4 RevIN.py

å¾ˆå¥½ï¼Œä½ è´´çš„æ˜¯ **RevIN (Reversible Instance Normalization)** æ¨¡å—çš„å®ç°ã€‚è¿™ä¸ªåœ¨å¾ˆå¤šæ—¶é—´åºåˆ—é¢„æµ‹æ¨¡å‹é‡Œï¼ˆåŒ…æ‹¬ DUETã€TimeXer ç­‰ï¼‰éƒ½ä¼šå‡ºç°ï¼Œç”¨æ¥**æ¶ˆé™¤ä¸åŒæ—¶é—´åºåˆ—ä¹‹é—´çš„åˆ†å¸ƒå·®å¼‚**ã€‚æˆ‘ç»™ä½ åˆ†å—è®²ä¸€ä¸‹å®ƒçš„é€»è¾‘å’Œæ„ä¹‰ã€‚

------

### 1. æ¨¡å—ä½œç”¨

RevIN çš„ä¸»è¦æ€æƒ³æ˜¯ï¼š

- åœ¨ **è¾“å…¥æ—¶ (norm)**ï¼šå¯¹æ¯ä¸ªæ ·æœ¬ã€æ¯ä¸ªé€šé“ï¼ˆfeatureï¼‰è¿›è¡Œ **å½’ä¸€åŒ–**ï¼Œæ¶ˆé™¤æ•°å€¼èŒƒå›´å’Œåˆ†å¸ƒçš„å·®å¼‚ï¼Œä½¿æ¨¡å‹æ›´å®¹æ˜“å­¦ä¹ ã€‚
- åœ¨ **è¾“å‡ºæ—¶ (denorm)**ï¼šæŠŠé¢„æµ‹ç»“æœå†è¿˜åŸåˆ°åŸå§‹çš„æ•°å€¼åˆ†å¸ƒï¼Œä¿è¯ç»“æœå¯è§£é‡Šã€‚

å®ƒçš„â€œReversibleâ€å°±æ˜¯æŒ‡è¿™ç§ **å½’ä¸€åŒ– â†” åå½’ä¸€åŒ–** çš„è¿‡ç¨‹ã€‚

------

### 2. ä»£ç è§£æ

#### åˆå§‹åŒ–

```python
def __init__(self, num_features, eps=1e-5, affine=True, subtract_last=False):
    self.num_features = num_features   # é€šé“æ•° D
    self.eps = eps                     # æ•°å€¼ç¨³å®šé¡¹
    self.affine = affine               # æ˜¯å¦å¼•å…¥å¯å­¦ä¹ ä»¿å°„å‚æ•°
    self.subtract_last = subtract_last # æ˜¯å¦å‡å»æœ€åä¸€ä¸ªå€¼
    if self.affine:
        self._init_params()            # åˆå§‹åŒ– scale å’Œ bias
```

- `affine=True` æ—¶ï¼Œä¼šå­¦ä¸€ä¸ª `weight` å’Œ `bias`ï¼Œç±»ä¼¼äº BatchNorm çš„å¯å­¦ä¹ ç¼©æ”¾å’Œåç§»ã€‚
- `subtract_last=True` ä¸»è¦ç”¨äº **éå¹³ç¨³åºåˆ—**ï¼ˆæ¯”å¦‚é£é€Ÿã€ä»·æ ¼ï¼‰ï¼Œå¯ä»¥è®©æ¨¡å‹æ›´èšç„¦äºå˜åŒ–é‡è€Œä¸æ˜¯ç»å¯¹å€¼ã€‚

------

#### å‰å‘ä¼ æ’­

```python
def forward(self, x, mode: str):
    if mode == 'norm':
        self._get_statistics(x)
        x = self._normalize(x)
    elif mode == 'denorm':
        x = self._denormalize(x)
    return x
```

åˆ†ä¸¤ç§æ¨¡å¼ï¼š

- `"norm"`ï¼šæ ‡å‡†åŒ–è¾“å…¥ï¼ˆè®­ç»ƒæˆ–æ¨ç†å‰ä½¿ç”¨ï¼‰ã€‚
- `"denorm"`ï¼šæŠŠé¢„æµ‹ç»“æœè¿˜åŸåˆ°åŸå§‹å°ºåº¦ã€‚

------

#### ç»Ÿè®¡é‡è®¡ç®—

```python
def _get_statistics(self, x):
    dim2reduce = tuple(range(1, x.ndim - 1))  # æ²¿æ—¶é—´ç»´åº¦æ±‚å‡å€¼/æ–¹å·®
    if self.subtract_last:
        self.last = x[:, -1, :].unsqueeze(1) # è®°ä½æœ€åæ—¶åˆ»
    else:
        self.mean = torch.mean(x, dim=dim2reduce, keepdim=True).detach()
    self.stdev = torch.sqrt(torch.var(x, dim=dim2reduce, keepdim=True, unbiased=False) + self.eps).detach()
```

- å¯¹è¾“å…¥ `x: [B, L, D]`ï¼Œåœ¨ `L` ç»´åº¦ä¸Šæ±‚å‡å€¼å’Œæ ‡å‡†å·®ã€‚
- `subtract_last=True` æ—¶ï¼Œç”¨æœ€åä¸€ä¸ªå€¼ä»£æ›¿å‡å€¼ï¼ˆå¯¹åº”â€œå»è¶‹åŠ¿â€æ“ä½œï¼‰ã€‚
- `.detach()` çš„ä½œç”¨æ˜¯é˜²æ­¢ç»Ÿè®¡é‡è¿›å…¥åå‘ä¼ æ’­ã€‚

------

#### å½’ä¸€åŒ–

```python
def _normalize(self, x):
    if self.subtract_last:
        x = x - self.last
    else:
        x = x - self.mean
    x = x / self.stdev
    if self.affine:
        x = x * self.affine_weight
        x = x + self.affine_bias
    return x
```

å°±æ˜¯æ ‡å‡†åŒ–å…¬å¼ï¼š

$x' = \frac{x - \mu}{\sigma}$

åŠ ä¸Šå¯å­¦ä¹ çš„ `scale` å’Œ `bias`ã€‚

------

#### åå½’ä¸€åŒ–

```python
def _denormalize(self, x):
    if self.affine:
        x = x - self.affine_bias
        x = x / (self.affine_weight + self.eps * self.eps)
    x = x * self.stdev
    if self.subtract_last:
        x = x + self.last
    else:
        x = x + self.mean
    return x
```

å°±æ˜¯åå‘æ“ä½œï¼š

$x = (x' - b) / w \times \sigma + \mu$

------

### 3. ä¸¾ä¾‹è¯´æ˜

å‡è®¾è¾“å…¥ `x: [32, 96, 7]`ï¼ˆ32 ä¸ªæ ·æœ¬ï¼Œåºåˆ—é•¿åº¦ 96ï¼Œå˜é‡æ•° 7ï¼‰ã€‚

1. **norm é˜¶æ®µ**ï¼š
   - å…ˆæŒ‰æ—¶é—´ç»´åº¦è®¡ç®—æ¯ä¸ªé€šé“çš„å‡å€¼ã€æ ‡å‡†å·®ã€‚
   - å¾—åˆ°å½’ä¸€åŒ–åçš„åºåˆ— `x_norm`ï¼Œå‡å€¼ 0ï¼Œæ–¹å·® 1ã€‚
2. **è®­ç»ƒæ¨¡å‹**ï¼š
   - æ¨¡å‹è¾“å…¥ `x_norm`ï¼Œè¾“å‡ºé¢„æµ‹ç»“æœ `y_norm`ã€‚
3. **denorm é˜¶æ®µ**ï¼š
   - æŠŠ `y_norm` è¿˜åŸå›åŸå§‹å°ºåº¦ï¼Œå¾—åˆ°é¢„æµ‹å€¼ `y_pred`ã€‚

è¿™æ ·æ¨¡å‹å°±èƒ½åœ¨æ ‡å‡†åŒ–ç©ºé—´å­¦ä¹ ï¼Œåˆèƒ½è¾“å‡ºç‰©ç†ä¸Šæœ‰æ„ä¹‰çš„ç»“æœã€‚

------

âš¡æ€»ç»“ï¼š

- `encoder` ä½ ä¹‹å‰è´´çš„ä¸»è¦æ˜¯ **èšç±» (TCM/CCM)**ã€‚
- `RevIN` ä¸»è¦æ˜¯ **é¢„å¤„ç† & åå¤„ç†æ¨¡å—**ï¼Œè®©ç¥ç»ç½‘ç»œåœ¨ç¨³å®šåˆ†å¸ƒä¸‹å­¦ä¹ ã€‚
- å®ƒä»¬ä¸¤ä¸ªæ˜¯ **ä¸åŒåŠŸèƒ½çš„ç»„ä»¶**ï¼šä¸€ä¸ªåœ¨ **å»ºæ¨¡æ ¸å¿ƒ**ï¼Œä¸€ä¸ªåœ¨ **è¾“å…¥/è¾“å‡ºè§„èŒƒåŒ–**ã€‚

è¦ä¸è¦æˆ‘å¸®ä½ æ•´ç†ä¸€ä¸ª **DUET çš„æ¨¡å—è¡¨**ï¼ˆæ¯”å¦‚ `encoder` = TCMï¼Œ`RevIN` = æ•°æ®å½’ä¸€åŒ–å·¥å…·ï¼Œ`linear_extractor_cluster` = æ¨¡å¼æå–â€¦ï¼‰ï¼Œè¿™æ ·ä½ èƒ½å¿«é€ŸçŸ¥é“æ¯ä¸ªæ–‡ä»¶çš„åŠŸèƒ½ï¼Ÿ



## 5 linear_extractor_cluster.py

ä½ è´´çš„è¿™ä»½ä»£ç å®ç°äº† **â€œç¨€ç–é—¨æ§çš„ä¸“å®¶ç½‘ç»œï¼ˆSparse Gated Mixture of Experts, MoEï¼‰â€**ï¼Œç»“åˆäº† Autoformer/Linear_extractor åšæ—¶é—´åºåˆ—ç‰¹å¾æå–ï¼Œå¹¶ä¸”æ”¯æŒ RevIN å½’ä¸€åŒ–ã€‚ä¸‹é¢æˆ‘å¸®ä½ é€å—è®²è§£ï¼Œä¿è¯ä½ å¿«é€Ÿç†è§£æ ¸å¿ƒé€»è¾‘ã€‚

------

æ˜ç™½äº†ï¼Œæˆ‘ç»™ä½ æŠŠä½ è´´çš„æ•´ä¸ª `Linear_extractor_cluster` ä»¥åŠ `SparseDispatcher` ä»£ç æ¨¡å—**æ¯ä¸€æ®µéƒ½è¯¦ç»†è§£é‡Š**ï¼Œä¸ä»…è¯´æ˜ç±»åï¼Œè¿˜ä¼šè§£é‡Šé‡Œé¢æ¯è¡Œæˆ–æ¯å—çš„ä½œç”¨ã€‚ä¸‹é¢æ˜¯å®Œæ•´è§£æï¼š

------

### 1ï¸âƒ£ SparseDispatcher

```python
class SparseDispatcher(object):
    """Helper for implementing a mixture of experts."""
```

- **ä½œç”¨**ï¼šè¾…åŠ©å®ç°ç¨€ç–é—¨æ§ MoEï¼ˆMixture of Expertsï¼‰
- **åŠŸèƒ½**ï¼š
  - å°†è¾“å…¥ batch åˆ†å‘ç»™å„ä¸ªä¸“å®¶ï¼ˆdispatchï¼‰
  - å°†å„ä¸“å®¶è¾“å‡ºæŒ‰ gate æƒé‡åˆå¹¶ï¼ˆcombineï¼‰

------

#### åˆå§‹åŒ–

```python
def __init__(self, num_experts, gates):
    self._gates = gates
    self._num_experts = num_experts
```

- `num_experts`ï¼šä¸“å®¶æ•°é‡
- `gates`ï¼šæ¯ä¸ªæ ·æœ¬å¯¹åº”æ¯ä¸ªä¸“å®¶çš„æƒé‡ï¼Œshape `[batch_size, num_experts]`

```python
sorted_experts, index_sorted_experts = torch.nonzero(gates).sort(0)
_, self._expert_index = sorted_experts.split(1, dim=1)
self._batch_index = torch.nonzero(gates)[index_sorted_experts[:, 1], 0]
self._part_sizes = (gates > 0).sum(0).tolist()
```

- æ‰¾åˆ°éé›¶ gate çš„ä¸“å®¶ç´¢å¼•å’Œ batch ç´¢å¼•
- `_part_sizes`ï¼šæ¯ä¸ªä¸“å®¶å®é™…æ¥æ”¶çš„æ ·æœ¬æ•°é‡

```python
gates_exp = gates[self._batch_index.flatten()]
self._nonzero_gates = torch.gather(gates_exp, 1, self._expert_index)
```

- `_nonzero_gates` ä¿å­˜æ¯ä¸ªä¸“å®¶å¯¹åº”æ ·æœ¬çš„ gate æƒé‡ï¼Œç”¨äºåˆå¹¶è¾“å‡º

------

#### dispatch

```python
def dispatch(self, inp):
    inp_exp = inp[self._batch_index].squeeze(1)
    return torch.split(inp_exp, self._part_sizes, dim=0)
```

- **ä½œç”¨**ï¼šæŠŠè¾“å…¥ batch æŒ‰ gate åˆ†é…ç»™å„ä¸ªä¸“å®¶
- è¿”å› `[num_experts]` ä¸ªå° batchï¼Œæ¯ä¸ªä¸“å®¶åªæ¥æ”¶å¯¹åº”æ ·æœ¬

------

#### combine

```python
def combine(self, expert_out, multiply_by_gates=True):
    stitched = torch.cat(expert_out, 0)
    if multiply_by_gates:
        stitched = torch.einsum("i...,ij->i...", stitched, self._nonzero_gates)
```

- **ä½œç”¨**ï¼šåˆå¹¶ä¸“å®¶è¾“å‡º
- `einsum` å¯¹æ¯ä¸ªæ ·æœ¬ä¹˜ä»¥ gate æƒé‡

```python
shape = list(expert_out[-1].shape)
shape[0] = self._gates.size(0)
zeros = torch.zeros(*shape, requires_grad=True, device=stitched.device)
combined = zeros.index_add(0, self._batch_index, stitched.float())
return combined
```

- ä½¿ç”¨ `_batch_index` æŠŠ stitched è¾“å‡ºç´¯åŠ åˆ°å¯¹åº”æ ·æœ¬ä½ç½®ï¼Œå¾—åˆ°æœ€ç»ˆ batch è¾“å‡º

------

#### expert_to_gates

```python
def expert_to_gates(self):
    return torch.split(self._nonzero_gates, self._part_sizes, dim=0)
```

- è¿”å›æ¯ä¸ªä¸“å®¶çš„ gate æƒé‡ï¼Œç”¨äºåˆ†å‘è®¡ç®—

------

### 2ï¸âƒ£ Linear_extractor_cluster

```python
class Linear_extractor_cluster(nn.Module):
    """Sparsely gated mixture of experts layer with Linear_extractor experts."""
```

- **åŠŸèƒ½**ï¼šå®ç°ä¸€ä¸ª MoE å±‚ï¼Œæ¯ä¸ªä¸“å®¶æ˜¯ `Linear_extractor`ï¼ˆæå–è¶‹åŠ¿+å­£èŠ‚æ€§ï¼‰
- **ç‰¹ç‚¹**ï¼š
  - æ¯ä¸ªæ ·æœ¬åªé€‰æ‹© top-k ä¸ªä¸“å®¶
  - æ”¯æŒè®­ç»ƒæ—¶åŠ å™ªå£°çš„ gateï¼ˆnoisy gatingï¼‰
  - æ”¯æŒ RevIN è¾“å…¥å½’ä¸€åŒ–

------

#### åˆå§‹åŒ–

```python
self.noisy_gating = config.noisy_gating
self.num_experts = config.num_experts
self.input_size = config.seq_len
self.k = config.k
self.experts = nn.ModuleList([expert(config) for _ in range(self.num_experts)])
self.W_h = nn.Parameter(torch.eye(self.num_experts))
self.gate = encoder(config)
self.noise = encoder(config)
self.n_vars = config.enc_in
self.revin = RevIN(self.n_vars)
self.CI = config.CI
self.softplus = nn.Softplus()
self.softmax = nn.Softmax(1)
self.register_buffer("mean", torch.tensor([0.0]))
self.register_buffer("std", torch.tensor([1.0]))
assert self.k <= self.num_experts
```

- åˆå§‹åŒ–æ‰€æœ‰å‚æ•°å’Œç»„ä»¶ï¼š
  - `experts`ï¼šæ¯ä¸ªä¸“å®¶éƒ½æ˜¯ `Linear_extractor`
  - `gate` & `noise`ï¼šè®¡ç®— gate æƒé‡å’Œå™ªå£°
  - `RevIN`ï¼šå½’ä¸€åŒ–è¾“å…¥
  - `W_h`ï¼šä¸“å®¶è¾“å‡ºçº¿æ€§ç»„åˆçŸ©é˜µ
  - `softplus` & `softmax`ï¼šç”¨äº gate è®¡ç®—

------

#### cv_squared

```python
def cv_squared(self, x):
    eps = 1e-10
    if x.shape[0] == 1:
        return torch.tensor([0], device=x.device, dtype=x.dtype)
    return x.float().var() / (x.float().mean() ** 2 + eps)
```

- **ä½œç”¨**ï¼šè®¡ç®—æ–¹å·®/å‡å€¼Â²ï¼Œç”¨ä½œè´Ÿè½½å‡è¡¡ loss
- é˜²æ­¢æŸäº›ä¸“å®¶è¿‡è½½æˆ–é—²ç½®

------

#### _gates_to_load

```python
def _gates_to_load(self, gates):
    return (gates > 0).sum(0)
```

- **ä½œç”¨**ï¼šè®¡ç®—æ¯ä¸ªä¸“å®¶å®é™…æ¥æ”¶çš„æ ·æœ¬æ•°

------

#### _prob_in_top_k

```python
def _prob_in_top_k(self, clean_values, noisy_values, noise_stddev, noisy_top_values):
    ...
```

- **ä½œç”¨**ï¼šè®¡ç®—å™ªå£°ä¸‹æ¯ä¸ªä¸“å®¶è¿›å…¥ top-k çš„æ¦‚ç‡ï¼Œç”¨äºæ¢¯åº¦å›ä¼ 
- æ¥æºè®ºæ–‡ï¼š[Noisy Top-k Gating](https://arxiv.org/abs/1701.06538)

------

#### noisy_top_k_gating

```python
def noisy_top_k_gating(self, x, train, noise_epsilon=1e-2):
    clean_logits = self.gate(x)
    if self.noisy_gating and train:
        raw_noise_stddev = self.noise(x)
        noise_stddev = self.softplus(raw_noise_stddev) + noise_epsilon
        noise = torch.randn_like(clean_logits)
        noisy_logits = clean_logits + (noise * noise_stddev)
        logits = noisy_logits @ self.W_h
    else:
        logits = clean_logits

    logits = self.softmax(logits)
    top_logits, top_indices = logits.topk(min(self.k + 1, self.num_experts), dim=1)
    top_k_logits = top_logits[:, : self.k]
    top_k_indices = top_indices[:, : self.k]
    top_k_gates = top_k_logits / (top_k_logits.sum(1, keepdim=True) + 1e-6)

    zeros = torch.zeros_like(logits, requires_grad=True)
    gates = zeros.scatter(1, top_k_indices, top_k_gates)

    if self.noisy_gating and self.k < self.num_experts and train:
        load = (self._prob_in_top_k(clean_logits, noisy_logits, noise_stddev, top_logits)).sum(0)
    else:
        load = self._gates_to_load(gates)

    return gates, load
```

- **åŠŸèƒ½**ï¼š
  1. è®¡ç®—æ¯ä¸ªæ ·æœ¬ top-k ä¸“å®¶
  2. å½’ä¸€åŒ– gate æƒé‡
  3. å¯é€‰è®­ç»ƒé˜¶æ®µåŠ å™ªå£°
  4. è¾“å‡º loadï¼ˆæ¯ä¸ªä¸“å®¶çš„æ ·æœ¬æ•°é‡ï¼‰

------

#### forward

```python
def forward(self, x, loss_coef=1):
    gates, load = self.noisy_top_k_gating(x, self.training)
    importance = gates.sum(0)
    loss = self.cv_squared(importance) + self.cv_squared(load)
    loss *= loss_coef

    dispatcher = SparseDispatcher(self.num_experts, gates)

    if self.CI:
        x_norm = rearrange(x, "(x y) l c -> x l (y c)", y=self.n_vars)
        x_norm = self.revin(x_norm, "norm")
        x_norm = rearrange(x_norm, "x l (y c) -> (x y) l c", y=self.n_vars)
    else:
        x_norm = self.revin(x, "norm")

    expert_inputs = dispatcher.dispatch(x_norm)
    gates = dispatcher.expert_to_gates()
    expert_outputs = [self.experts[i](expert_inputs[i]) for i in range(self.num_experts)]
    y = dispatcher.combine(expert_outputs)

    return y, loss
```

- **æµç¨‹**ï¼š
  1. è®¡ç®— gate + load
  2. è®¡ç®—è´Ÿè½½å‡è¡¡ loss
  3. å¯¹è¾“å…¥åš RevIN å½’ä¸€åŒ–
  4. åˆ†å‘ç»™ä¸“å®¶
  5. æ¯ä¸ªä¸“å®¶è¾“å‡º
  6. åˆå¹¶ä¸“å®¶è¾“å‡º
  7. è¿”å›é¢„æµ‹ç»“æœ + loss

------

âœ… æ€»ç»“ï¼š

- `SparseDispatcher` è´Ÿè´£è¾“å…¥åˆ†å‘ & è¾“å‡ºåˆå¹¶
- `Linear_extractor_cluster` å®ç° **ç¨€ç–é—¨æ§ MoE å±‚**
- æ¯ä¸ªä¸“å®¶æ˜¯ `Linear_extractor` â†’ æ‹†åˆ†è¶‹åŠ¿ + å­£èŠ‚æ€§
- åŠ ä¸Š **RevIN + Noisy Top-k Gating + è´Ÿè½½å‡è¡¡ loss** â†’ æé«˜æ¨¡å‹æ³›åŒ–å’Œè®­ç»ƒç¨³å®šæ€§

------



### å¤„ç†è¿‡ç¨‹

æ˜ç™½äº†ï¼Œæˆ‘å¸®ä½ æ¢³ç†ä¸€ä¸‹ **æ•´ä¸ª Linear_extractor_cluster + SparseDispatcher + Linear_extractor çš„æµç¨‹**ï¼Œå‡è®¾ä½ è¾“å…¥æ˜¯ `[batch, seq_len, channels] = [32, 96, 7]`ï¼Œé¢„æµ‹ 24 æ­¥ï¼ˆpred_len=24ï¼‰ã€‚æˆ‘ä¼šæŒ‰æ•°æ®æµã€æ¨¡å—åˆ†è§£ï¼Œè®²æ¸…æ¯ä¸€æ­¥åšäº†ä»€ä¹ˆã€‚

------

#### 1ï¸âƒ£ è¾“å…¥æ•°æ®

- **å½¢çŠ¶**ï¼š`x_enc = [32, 96, 7]`
  - batch = 32
  - seq_len = 96ï¼ˆå†å²é•¿åº¦ï¼‰
  - channels = 7ï¼ˆç‰¹å¾ç»´åº¦ï¼Œä¾‹å¦‚é£é€Ÿã€åŠŸç‡ç­‰ï¼‰
- **ç›®æ ‡**ï¼šé¢„æµ‹æœªæ¥ 24 ä¸ªæ—¶é—´æ­¥çš„åºåˆ—ï¼Œè¾“å‡º `[32, 24, 7]`ã€‚

------

#### 2ï¸âƒ£ RevIN å½’ä¸€åŒ–

- **æ¨¡å—**ï¼š`RevIN`
- **åŠŸèƒ½**ï¼š
  - å¯¹æ¯ä¸ªæ ·æœ¬æ¯ä¸ªé€šé“è¿›è¡Œå‡å€¼-æ–¹å·®å½’ä¸€åŒ–
  - å¦‚æœ `CI=True`ï¼Œå…ˆ reshape `[batch*channels, seq_len, 1]` åšå½’ä¸€åŒ–ï¼Œå†æ¢å¤åŸ shape
- **è¾“å‡ºå½¢çŠ¶**ï¼šä»ç„¶ `[32, 96, 7]`ï¼Œä½†æ˜¯å€¼è¢«å½’ä¸€åŒ–åˆ° 0 å‡å€¼ã€å•ä½æ–¹å·®é™„è¿‘

------

#### 3ï¸âƒ£ Gate è®¡ç®—ï¼ˆNoisy Top-k Gatingï¼‰

- **æ¨¡å—**ï¼š`encoder` â†’ `noisy_top_k_gating`
- **è¾“å…¥**ï¼šå½’ä¸€åŒ–åçš„ `[32, 96, 7]`
- **å¤„ç†**ï¼š
  1. å…ˆè®¡ç®—æ¯ä¸ªæ ·æœ¬å¯¹åº”æ¯ä¸ªä¸“å®¶çš„ `logits`ï¼Œå½¢çŠ¶ `[32, num_experts]`
  2. å¦‚æœè®­ç»ƒé˜¶æ®µ `noisy_gating=True`ï¼Œåœ¨ logits ä¸ŠåŠ å™ªå£°
  3. softmax å¾—åˆ°æ¦‚ç‡
  4. é€‰æ‹© **top-k** ä¸“å®¶ä½œä¸ºå½“å‰æ ·æœ¬çš„é¢„æµ‹è·¯å¾„
- **è¾“å‡º**ï¼š
  - `gates`ï¼š[32, num_experts]ï¼Œæ¯è¡Œåªæœ‰ k ä¸ªéé›¶å€¼
  - `load`ï¼š[num_experts]ï¼Œæ¯ä¸ªä¸“å®¶å®é™…è¢«é€‰ä¸­çš„æ ·æœ¬æ•°

------

#### 4ï¸âƒ£ SparseDispatcher åˆ†å‘è¾“å…¥

- **æ¨¡å—**ï¼š`SparseDispatcher(dispatch)`
- **è¾“å…¥**ï¼šå½’ä¸€åŒ–åçš„ `[32, 96, 7]`
- **å¤„ç†**ï¼š
  - æ ¹æ® gateï¼ŒæŠŠæ¯ä¸ªæ ·æœ¬åˆ†å‘åˆ°å¯¹åº”çš„ top-k ä¸“å®¶
  - å½¢æˆæ¯ä¸ªä¸“å®¶çš„å° batch
- **è¾“å‡º**ï¼š
  - `[num_experts]` ä¸ªå° tensorï¼Œä¾‹å¦‚æ¯ä¸ª `[num_samples_for_expert, 96, 7]`

------

#### 5ï¸âƒ£ æ¯ä¸ªä¸“å®¶å¤„ç†ï¼ˆLinear_extractorï¼‰

- **æ¨¡å—**ï¼š`Linear_extractor`
- **è¾“å…¥**ï¼šæ¯ä¸ªä¸“å®¶çš„å° batch `[batch_expert, 96, 7]`
- **æ­¥éª¤**ï¼š
  1. **åºåˆ—åˆ†è§£**ï¼š`series_decomp`
     - è¾“å…¥ `[batch_expert, 96, 7]` â†’ æ‹†æˆï¼š
       - `seasonal_init`ï¼š[batch_expert, 96, 7]ï¼ˆæ®‹å·®/å­£èŠ‚æ€§ï¼‰
       - `trend_init`ï¼š[batch_expert, 96, 7]ï¼ˆè¶‹åŠ¿ï¼‰
  2. **Linear æ˜ å°„**ï¼š
     - å°†æ¯ä¸ªæ—¶é—´åºåˆ—ï¼ˆé•¿åº¦ 96ï¼‰é€šè¿‡çº¿æ€§å±‚æ˜ å°„åˆ°é¢„æµ‹é•¿åº¦ 24
     - `seasonal_output` + `trend_output` â†’ `[batch_expert, 24, 7]`
- **è¾“å‡º**ï¼šæ¯ä¸ªä¸“å®¶çš„å° batch `[batch_expert, 24, 7]`

------

#### 6ï¸âƒ£ SparseDispatcher åˆå¹¶ä¸“å®¶è¾“å‡º

- **æ¨¡å—**ï¼š`SparseDispatcher(combine)`
- **è¾“å…¥**ï¼šæ‰€æœ‰ä¸“å®¶çš„å° batch è¾“å‡º + gates æƒé‡
- **å¤„ç†**ï¼š
  - å¯¹æ¯ä¸ªæ ·æœ¬ï¼Œæ ¹æ® gate æƒé‡åŠ æƒæ±‚å’Œ top-k ä¸“å®¶è¾“å‡º
- **è¾“å‡º**ï¼š
  - `[32, 24, 7]`ï¼Œå’Œè¾“å…¥ batch å¯¹é½ï¼Œå¾—åˆ°æœ€ç»ˆé¢„æµ‹

------

#### 7ï¸âƒ£ è´Ÿè½½å‡è¡¡ loss

- **è®¡ç®—**ï¼š
  - importance = æ¯ä¸ªä¸“å®¶ gate æ€»å’Œ
  - load = æ¯ä¸ªä¸“å®¶å®é™…æ¥æ”¶æ ·æœ¬æ•°
  - `loss = cv_squared(importance) + cv_squared(load)`
- **ç”¨é€”**ï¼š
  - è®­ç»ƒæ—¶é¼“åŠ±æ¯ä¸ªä¸“å®¶ä½¿ç”¨å‡åŒ€ï¼Œä¸è®©æŸäº›ä¸“å®¶é—²ç½®æˆ–è¿‡è½½

------

#### 8ï¸âƒ£ æ€»ä½“æ•°æ®æµç¤ºæ„

```
Input x: [32, 96, 7]
      â”‚
      â–¼
RevIN (å½’ä¸€åŒ–) -> [32, 96, 7]
      â”‚
      â–¼
Top-k Gate (encoder + noisy gating) -> gates [32, num_experts]
      â”‚
      â–¼
SparseDispatcher.dispatch -> æ¯ä¸ªä¸“å®¶å° batch
      â”‚
      â–¼
Linear_extractor (trend+seasonal) -> [batch_expert, 24, 7]
      â”‚
      â–¼
SparseDispatcher.combine -> [32, 24, 7] (é¢„æµ‹è¾“å‡º)
```

------

âœ… æ€»ç»“ï¼š

- **è¾“å…¥ `[32,96,7]` â†’ å½’ä¸€åŒ– â†’ top-k gate â†’ åˆ†å‘ç»™ä¸“å®¶ â†’ æ¯ä¸ªä¸“å®¶æå– trend+seasonal â†’ è¾“å‡º 24 æ­¥é¢„æµ‹ â†’ åˆå¹¶ top-k è¾“å‡º â†’ `[32,24,7]`**
- è¿™ä¸ªæµç¨‹**æŠŠå†å²åºåˆ—åˆ†è§£ + ä¸“å®¶ç¨€ç–ç»„åˆ**ï¼Œå…¼é¡¾å±€éƒ¨æ¨¡å¼å­¦ä¹ å’Œå…¨å±€å¤šæ ·æ€§

------

æˆ‘å¯ä»¥å¸®ä½ ç”»ä¸€å¼  **æµç¨‹å›¾**ï¼ŒæŠŠ `[32,96,7]` çš„è¾“å…¥åˆ° `[32,24,7]` çš„æ•´ä¸ª MoE + Linear_extractor å¤„ç†å¯è§†åŒ–ï¼Œå¸¦æ¯ä¸ªæ¨¡å—çš„å½¢çŠ¶ï¼Œéå¸¸ç›´è§‚ã€‚

ä½ å¸Œæœ›æˆ‘ç”»å—ï¼Ÿ



è¯¥æ¨¡å‹çš„è®¾è®¡çµæ„Ÿæ¥æºäºä»¥ä¸‹è®ºæ–‡ï¼š

- **Outrageously Large Neural Networks: Sparsely-Gated Mixture of Experts Layers**ï¼šæå‡ºäº†ç¨€ç–é—¨æ§ä¸“å®¶å±‚çš„æ¦‚å¿µï¼Œé€šè¿‡é€‰æ‹©æ€§åœ°æ¿€æ´»éƒ¨åˆ†ä¸“å®¶æ¥æé«˜æ¨¡å‹çš„è®¡ç®—æ•ˆç‡ã€‚
- **Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity**ï¼šä»‹ç»äº† Switch Transformer æ¨¡å‹ï¼Œé‡‡ç”¨äº†ç±»ä¼¼çš„ç¨€ç–ä¸“å®¶æœºåˆ¶ã€‚